{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "from copy import copy\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Allow augmentation transform for training set, no augementation for val/test set\n",
    "# Normalize(mean, std, inplace=False)\n",
    "# mean, std = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "mean, std = (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "preprocess_augment = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)])\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)])\n",
    "\n",
    "# Download CIFAR-10 and split into training, validation, and test sets.\n",
    "# The copy of the training dataset after the split allows us to keep\n",
    "# the same training/validation split of the original training set but\n",
    "# apply different transforms to the training set and validation set.\n",
    "\n",
    "full_train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_train_dataset, [40000, 10000])\n",
    "train_dataset.dataset = copy(full_train_dataset)\n",
    "train_dataset.dataset.transform = preprocess_augment\n",
    "val_dataset.dataset.transform = preprocess\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=preprocess)\n",
    "\n",
    "# DataLoaders for the three datasets\n",
    "BATCH_SIZE=4\n",
    "NUM_WORKERS=2\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,shuffle=True , num_workers=NUM_WORKERS)\n",
    "val_dataloader   = torch.utils.data.DataLoader(val_dataset  , batch_size=BATCH_SIZE,shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_dataloader  = torch.utils.data.DataLoader(test_dataset , batch_size=BATCH_SIZE,shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "dataloaders = {'train': train_dataloader, 'val': val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda:1\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'num_classes' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1697174e96e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# ResNet18\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_BLOCK_BASIC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# Optimizer and loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_classes' is not defined"
     ]
    }
   ],
   "source": [
    "from myNetwork.myResNet import ResNet\n",
    "from trainer import trainer\n",
    "\n",
    "# Set device to GPU or CPU\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# ResNet18\n",
    "num_classes = 10\n",
    "model = ResNet(ResNet._BLOCK_BASIC, [2, 2, 2, 2], num_classes).to(device)\n",
    "# Optimizer and loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params_to_update = model.parameters()\n",
    "optimizer = optim.Adam(params_to_update, lr=0.01)\n",
    "\n",
    "t = trainer(device,criterion, optimizer)\n",
    "model = t.train(model, dataloaders, 1, 'resnet18_adam_0.01')\n",
    "t.test(model,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda:1\n",
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 1.8179 Acc: 0.3293\n",
      "Epoch time taken:  175.96193265914917\n",
      "val Loss: 1.6723 Acc: 0.4260\n",
      "Epoch time taken:  189.732736825943\n",
      "Training complete in 3m 10s\n",
      "Best val Acc: 0.426000\n"
     ]
    }
   ],
   "source": [
    "def SEResNet18(num_classes = 10):\n",
    "    return ResNet(ResNet._BLOCK_SEBASIC, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = SEResNet18().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params_to_update = model.parameters()\n",
    "# Now we'll use Adam optimization\n",
    "optimizer = optim.Adam(params_to_update, lr=0.01)\n",
    "t = trainer(device,criterion, optimizer)\n",
    "model = t.train(model, dataloaders, 1, 'seresnet18_adam_0.01')\n",
    "\n"
   ]
  }
 ]
}