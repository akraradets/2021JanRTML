{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1801.09573.pdf\n",
    "# Datasets\n",
    "# We have collected the 1000 images from the internet in 2 categories as muffin and Chihuahua\n",
    "# including 200 images from Oxford pet animal dataset [7] as shown in Figure 3. Please note here, all\n",
    "# resources as images from the internet is for research purpose only, we donâ€™t own any of them. ImageNet\n",
    "# [5] also includes 1750 Chihuahua and 1335 various type of muffin images already.\n",
    "\n",
    "# http://image-net.org/synset?wnid=n02085620 : 1750 Chihuahua\n",
    "# http://image-net.org/synset?wnid=n07690273 : 1335 muffins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://aviaryan.com/blog/gsoc/downloading-files-from-urls\n",
    "# import requests\n",
    "# def is_downloadable(url):\n",
    "#     \"\"\"\n",
    "#     Does the url contain a downloadable resource\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         h = requests.head(url, allow_redirects=True)\n",
    "#     except:\n",
    "#         return False\n",
    "#     header = h.headers\n",
    "#     content_type = header.get('content-type')\n",
    "#     if content_type == None:\n",
    "#         return False\n",
    "#     if 'text' in content_type.lower():\n",
    "#         return False\n",
    "#     if 'html' in content_type.lower():\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "# file1 = open('chihuahua.txt', 'r')\n",
    "# Lines = file1.readlines()\n",
    "# count = 0\n",
    "# for line in Lines:\n",
    "#     print(line)\n",
    "#     if(is_downloadable(line)):\n",
    "#         count += 1\n",
    "#         r = requests.get(line, allow_redirects=True)\n",
    "#         open(f'data/chihuahua/{count}.jpg', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file1 = open('muffin.txt', 'r')\n",
    "# Lines = file1.readlines()\n",
    "# count = 0\n",
    "# for line in Lines:\n",
    "#     print(line)\n",
    "#     if(is_downloadable(line)):\n",
    "#         count += 1\n",
    "#         r = requests.get(line, allow_redirects=True)\n",
    "#         open(f'data/muffin/{count}.jpg', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset ImageFolder\n    Number of datapoints: 1988\n    Root location: data/train\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "from copy import copy\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Allow augmentation transform for training set, no augementation for val/test set\n",
    "# Normalize(mean, std, inplace=False)\n",
    "# mean, std = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "mean, std = (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "preprocess_augment = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)])\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)])\n",
    "\n",
    "\n",
    "full_train_dataset = torchvision.datasets.ImageFolder('data/train')\n",
    "print(full_train_dataset)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_train_dataset, [1600, 1988-1600])\n",
    "train_dataset.dataset = copy(full_train_dataset)\n",
    "train_dataset.dataset.transform = preprocess_augment\n",
    "val_dataset.dataset.transform = preprocess\n",
    "\n",
    "# DataLoaders for the three datasets\n",
    "BATCH_SIZE=4\n",
    "NUM_WORKERS=2\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,shuffle=True , num_workers=NUM_WORKERS)\n",
    "val_dataloader   = torch.utils.data.DataLoader(val_dataset  , batch_size=BATCH_SIZE,shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# # get some random training images\n",
    "# dataiter = iter(train_dataloader)\n",
    "# images, labels = dataiter.next()\n",
    "# print(labels)\n",
    "# toshow = torchvision.utils.make_grid(images)\n",
    "# toshow = toshow / 2 + 0.5     # unnormalize\n",
    "# npimg = toshow.numpy()\n",
    "# plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "# plt.show()\n",
    "# classes = np.array(['chihuahua','muffin'])\n",
    "# print(labels, classes[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda:1\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (1): Sequential(\n",
       "      (0): SEBasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (se): SELayer(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=64, out_features=4, bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=4, out_features=64, bias=False)\n",
       "            (3): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (1): SEBasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (se): SELayer(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=64, out_features=4, bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=4, out_features=64, bias=False)\n",
       "            (3): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): SEBasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (se): SELayer(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=8, bias=False)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=8, out_features=128, bias=False)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): SEBasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (se): SELayer(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=8, bias=False)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=8, out_features=128, bias=False)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): SEBasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (se): SELayer(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=16, bias=False)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=16, out_features=256, bias=False)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): SEBasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (se): SELayer(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=16, bias=False)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=16, out_features=256, bias=False)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (conv5): Sequential(\n",
       "    (0): SEBasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (se): SELayer(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=32, bias=False)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=32, out_features=512, bias=False)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): SEBasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (se): SELayer(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=32, bias=False)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=32, out_features=512, bias=False)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (1): Flatten()\n",
       "    (2): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "dataloaders = {'train': train_dataloader, 'val': val_dataloader}\n",
    "\n",
    "from myNetwork.myResNet import ResNet\n",
    "from trainer import trainer\n",
    "\n",
    "def SEResNet18(num_classes = 10):\n",
    "    return ResNet(ResNet._BLOCK_SEBASIC, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = SEResNet18()\n",
    "model.load_state_dict(torch.load('result-20210129-104338/seresnet18_adam_0.01.pth'))\n",
    "model.classifier[2] = nn.Linear(512,2)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0/59:LR: 0.01\n",
      "----------\n",
      "train Loss: 0.3794 Acc: 0.8294\n",
      "Epoch time taken:  7.2649407386779785\n",
      "val Loss: 0.2332 Acc: 0.8918\n",
      "Epoch time taken:  8.395422458648682\n",
      "Epoch 1/59:LR: 0.01\n",
      "----------\n",
      "train Loss: 0.2488 Acc: 0.9025\n",
      "Epoch time taken:  6.999926567077637\n",
      "val Loss: 0.1990 Acc: 0.9149\n",
      "Epoch time taken:  8.068832635879517\n",
      "Epoch 2/59:LR: 0.01\n",
      "----------\n",
      "train Loss: 0.1951 Acc: 0.9269\n",
      "Epoch time taken:  7.136178731918335\n",
      "val Loss: 0.2252 Acc: 0.9021\n",
      "Epoch time taken:  8.171086311340332\n",
      "Epoch 3/59:LR: 0.01\n",
      "----------\n",
      "train Loss: 0.1778 Acc: 0.9294\n",
      "Epoch time taken:  7.13777756690979\n",
      "val Loss: 0.1986 Acc: 0.9021\n",
      "Epoch time taken:  8.165367126464844\n",
      "Epoch 4/59:LR: 0.01\n",
      "----------\n",
      "train Loss: 0.1650 Acc: 0.9350\n",
      "Epoch time taken:  7.021339416503906\n",
      "val Loss: 0.1912 Acc: 0.8995\n",
      "Epoch time taken:  8.055789232254028\n",
      "Epoch 5/59:LR: 0.01\n",
      "----------\n",
      "train Loss: 0.1542 Acc: 0.9431\n",
      "Epoch time taken:  7.03818678855896\n",
      "val Loss: 0.1969 Acc: 0.9201\n",
      "Epoch time taken:  8.069774627685547\n",
      "Epoch 6/59:LR: 0.01\n",
      "----------\n",
      "train Loss: 0.1289 Acc: 0.9469\n",
      "Epoch time taken:  7.0746049880981445\n",
      "val Loss: 0.2264 Acc: 0.9098\n",
      "Epoch time taken:  8.102429151535034\n",
      "Epoch 7/59:LR: 0.01\n",
      "----------\n",
      "train Loss: 0.1315 Acc: 0.9537\n",
      "Epoch time taken:  6.974340438842773\n",
      "val Loss: 0.2196 Acc: 0.9124\n",
      "Epoch time taken:  8.019014358520508\n",
      "Epoch 8/59:LR: 0.01\n",
      "----------\n",
      "train Loss: 0.1137 Acc: 0.9531\n",
      "Epoch time taken:  7.161525011062622\n",
      "val Loss: 0.1981 Acc: 0.9304\n",
      "Epoch time taken:  8.191205263137817\n",
      "Epoch 9/59:LR: 0.01\n",
      "----------\n",
      "train Loss: 0.1328 Acc: 0.9481\n",
      "Epoch time taken:  7.069777965545654\n",
      "val Loss: 0.1982 Acc: 0.9253\n",
      "Epoch time taken:  8.092375755310059\n",
      "Epoch 10/59:LR: 0.01\n",
      "----------\n",
      "train Loss: 0.1060 Acc: 0.9631\n",
      "Epoch time taken:  7.121400833129883\n",
      "val Loss: 0.1940 Acc: 0.9098\n",
      "Epoch time taken:  8.166183233261108\n",
      "Epoch 11/59:LR: 0.001\n",
      "----------\n",
      "train Loss: 0.0785 Acc: 0.9725\n",
      "Epoch time taken:  7.05527400970459\n",
      "val Loss: 0.1797 Acc: 0.9175\n",
      "Epoch time taken:  8.208142042160034\n",
      "Epoch 12/59:LR: 0.001\n",
      "----------\n",
      "train Loss: 0.0763 Acc: 0.9763\n",
      "Epoch time taken:  7.050307035446167\n",
      "val Loss: 0.1834 Acc: 0.9201\n",
      "Epoch time taken:  8.085347652435303\n",
      "Epoch 13/59:LR: 0.001\n",
      "----------\n",
      "train Loss: 0.0771 Acc: 0.9706\n",
      "Epoch time taken:  7.078200817108154\n",
      "val Loss: 0.1783 Acc: 0.9175\n",
      "Epoch time taken:  8.102636337280273\n",
      "Epoch 14/59:LR: 0.001\n",
      "----------\n",
      "train Loss: 0.0645 Acc: 0.9744\n",
      "Epoch time taken:  7.040393590927124\n",
      "val Loss: 0.1893 Acc: 0.9072\n",
      "Epoch time taken:  8.071919202804565\n",
      "Epoch 15/59:LR: 0.001\n",
      "----------\n",
      "train Loss: 0.0716 Acc: 0.9738\n",
      "Epoch time taken:  7.05532693862915\n",
      "val Loss: 0.1850 Acc: 0.9201\n",
      "Epoch time taken:  8.091727256774902\n",
      "Epoch 16/59:LR: 0.001\n",
      "----------\n",
      "train Loss: 0.0792 Acc: 0.9663\n",
      "Epoch time taken:  7.03971791267395\n",
      "val Loss: 0.1675 Acc: 0.9201\n",
      "Epoch time taken:  8.069294929504395\n",
      "Epoch 17/59:LR: 0.001\n",
      "----------\n",
      "train Loss: 0.0640 Acc: 0.9775\n",
      "Epoch time taken:  7.03092098236084\n",
      "val Loss: 0.1763 Acc: 0.9278\n",
      "Epoch time taken:  8.06968641281128\n",
      "Epoch 18/59:LR: 0.001\n",
      "----------\n",
      "train Loss: 0.0735 Acc: 0.9694\n",
      "Epoch time taken:  7.0762550830841064\n",
      "val Loss: 0.1945 Acc: 0.9253\n",
      "Epoch time taken:  8.119093179702759\n",
      "Epoch 19/59:LR: 0.001\n",
      "----------\n",
      "train Loss: 0.0676 Acc: 0.9769\n",
      "Epoch time taken:  7.037475347518921\n",
      "val Loss: 0.2094 Acc: 0.9175\n",
      "Epoch time taken:  8.075071573257446\n",
      "Epoch 20/59:LR: 0.001\n",
      "----------\n",
      "train Loss: 0.0676 Acc: 0.9763\n",
      "Epoch time taken:  7.051567792892456\n",
      "val Loss: 0.1800 Acc: 0.9227\n",
      "Epoch time taken:  8.078675031661987\n",
      "Epoch 21/59:LR: 0.001\n",
      "----------\n",
      "train Loss: 0.0605 Acc: 0.9813\n",
      "Epoch time taken:  7.0299293994903564\n",
      "val Loss: 0.1816 Acc: 0.9175\n",
      "Epoch time taken:  8.064472198486328\n",
      "Epoch 22/59:LR: 0.001\n",
      "----------\n",
      "train Loss: 0.0696 Acc: 0.9756\n",
      "Epoch time taken:  7.071471929550171\n",
      "val Loss: 0.1787 Acc: 0.9227\n",
      "Epoch time taken:  8.125247716903687\n",
      "Epoch 23/59:LR: 0.0001\n",
      "----------\n",
      "train Loss: 0.0575 Acc: 0.9806\n",
      "Epoch time taken:  7.049889802932739\n",
      "val Loss: 0.1870 Acc: 0.9278\n",
      "Epoch time taken:  8.088250875473022\n",
      "Epoch 24/59:LR: 0.0001\n",
      "----------\n",
      "train Loss: 0.0639 Acc: 0.9775\n",
      "Epoch time taken:  7.100680589675903\n",
      "val Loss: 0.1827 Acc: 0.9253\n",
      "Epoch time taken:  8.1337890625\n",
      "Epoch 25/59:LR: 0.0001\n",
      "----------\n",
      "train Loss: 0.0485 Acc: 0.9850\n",
      "Epoch time taken:  7.060314178466797\n",
      "val Loss: 0.1826 Acc: 0.9227\n",
      "Epoch time taken:  8.10128664970398\n",
      "Epoch 26/59:LR: 0.0001\n",
      "----------\n",
      "train Loss: 0.0573 Acc: 0.9769\n",
      "Epoch time taken:  7.042152166366577\n",
      "val Loss: 0.1859 Acc: 0.9278\n",
      "Epoch time taken:  8.080488681793213\n",
      "Epoch 27/59:LR: 0.0001\n",
      "----------\n",
      "train Loss: 0.0560 Acc: 0.9788\n",
      "Epoch time taken:  7.054517507553101\n",
      "val Loss: 0.1805 Acc: 0.9201\n",
      "Epoch time taken:  8.092012405395508\n",
      "Epoch 28/59:LR: 0.0001\n",
      "----------\n",
      "train Loss: 0.0626 Acc: 0.9738\n",
      "Epoch time taken:  7.028908729553223\n",
      "val Loss: 0.1775 Acc: 0.9253\n",
      "Epoch time taken:  8.064610481262207\n",
      "Epoch 29/59:LR: 1e-05\n",
      "----------\n",
      "train Loss: 0.0567 Acc: 0.9806\n",
      "Epoch time taken:  7.039376735687256\n",
      "val Loss: 0.1793 Acc: 0.9278\n",
      "Epoch time taken:  8.069408655166626\n",
      "Epoch 30/59:LR: 1e-05\n",
      "----------\n",
      "train Loss: 0.0658 Acc: 0.9731\n",
      "Epoch time taken:  7.160036087036133\n",
      "val Loss: 0.1911 Acc: 0.9201\n",
      "Epoch time taken:  8.19949984550476\n",
      "Epoch 31/59:LR: 1e-05\n",
      "----------\n",
      "train Loss: 0.0647 Acc: 0.9719\n",
      "Epoch time taken:  7.072087049484253\n",
      "val Loss: 0.1769 Acc: 0.9253\n",
      "Epoch time taken:  8.105545282363892\n",
      "Epoch 32/59:LR: 1e-05\n",
      "----------\n",
      "train Loss: 0.0566 Acc: 0.9794\n",
      "Epoch time taken:  7.010193586349487\n",
      "val Loss: 0.1796 Acc: 0.9278\n",
      "Epoch time taken:  8.076503038406372\n",
      "Epoch 33/59:LR: 1e-05\n",
      "----------\n",
      "train Loss: 0.0528 Acc: 0.9819\n",
      "Epoch time taken:  7.201112985610962\n",
      "val Loss: 0.1826 Acc: 0.9253\n",
      "Epoch time taken:  8.228991270065308\n",
      "Epoch 34/59:LR: 1e-05\n",
      "----------\n",
      "train Loss: 0.0564 Acc: 0.9788\n",
      "Epoch time taken:  7.017324686050415\n",
      "val Loss: 0.1865 Acc: 0.9253\n",
      "Epoch time taken:  8.050605058670044\n",
      "Epoch 35/59:LR: 1.0000000000000002e-06\n",
      "----------\n",
      "train Loss: 0.0610 Acc: 0.9763\n",
      "Epoch time taken:  7.0469887256622314\n",
      "val Loss: 0.1837 Acc: 0.9201\n",
      "Epoch time taken:  8.096769571304321\n",
      "Epoch 36/59:LR: 1.0000000000000002e-06\n",
      "----------\n",
      "train Loss: 0.0661 Acc: 0.9775\n",
      "Epoch time taken:  7.0691306591033936\n",
      "val Loss: 0.1804 Acc: 0.9201\n",
      "Epoch time taken:  8.102747440338135\n",
      "Epoch 37/59:LR: 1.0000000000000002e-06\n",
      "----------\n",
      "train Loss: 0.0642 Acc: 0.9725\n",
      "Epoch time taken:  7.038508892059326\n",
      "val Loss: 0.1819 Acc: 0.9201\n",
      "Epoch time taken:  8.070872783660889\n",
      "Epoch 38/59:LR: 1.0000000000000002e-06\n",
      "----------\n",
      "train Loss: 0.0608 Acc: 0.9794\n",
      "Epoch time taken:  7.0309388637542725\n",
      "val Loss: 0.1811 Acc: 0.9201\n",
      "Epoch time taken:  8.065832138061523\n",
      "Epoch 39/59:LR: 1.0000000000000002e-06\n",
      "----------\n",
      "train Loss: 0.0509 Acc: 0.9825\n",
      "Epoch time taken:  7.008861064910889\n",
      "val Loss: 0.1828 Acc: 0.9227\n",
      "Epoch time taken:  8.042654275894165\n",
      "Epoch 40/59:LR: 1.0000000000000002e-06\n",
      "----------\n",
      "train Loss: 0.0517 Acc: 0.9838\n",
      "Epoch time taken:  7.036646366119385\n",
      "val Loss: 0.1876 Acc: 0.9253\n",
      "Epoch time taken:  8.065882921218872\n",
      "Epoch 41/59:LR: 1.0000000000000002e-07\n",
      "----------\n",
      "train Loss: 0.0547 Acc: 0.9806\n",
      "Epoch time taken:  7.044218063354492\n",
      "val Loss: 0.1873 Acc: 0.9227\n",
      "Epoch time taken:  8.080600023269653\n",
      "Epoch 42/59:LR: 1.0000000000000002e-07\n",
      "----------\n",
      "train Loss: 0.0522 Acc: 0.9806\n",
      "Epoch time taken:  7.142462253570557\n",
      "val Loss: 0.2016 Acc: 0.9124\n",
      "Epoch time taken:  8.178154230117798\n",
      "Epoch 43/59:LR: 1.0000000000000002e-07\n",
      "----------\n",
      "train Loss: 0.0612 Acc: 0.9775\n",
      "Epoch time taken:  7.085064172744751\n",
      "val Loss: 0.1894 Acc: 0.9227\n",
      "Epoch time taken:  8.190024375915527\n",
      "Epoch 44/59:LR: 1.0000000000000002e-07\n",
      "----------\n",
      "train Loss: 0.0626 Acc: 0.9769\n",
      "Epoch time taken:  7.210788726806641\n",
      "val Loss: 0.1885 Acc: 0.9201\n",
      "Epoch time taken:  8.247560262680054\n",
      "Epoch 45/59:LR: 1.0000000000000002e-07\n",
      "----------\n",
      "train Loss: 0.0582 Acc: 0.9794\n",
      "Epoch time taken:  7.138638496398926\n",
      "val Loss: 0.1847 Acc: 0.9253\n",
      "Epoch time taken:  8.179405212402344\n",
      "Epoch 46/59:LR: 1.0000000000000002e-07\n",
      "----------\n",
      "train Loss: 0.0534 Acc: 0.9819\n",
      "Epoch time taken:  7.046038627624512\n",
      "val Loss: 0.1905 Acc: 0.9201\n",
      "Epoch time taken:  8.229239702224731\n",
      "Epoch 47/59:LR: 1.0000000000000004e-08\n",
      "----------\n",
      "train Loss: 0.0506 Acc: 0.9819\n",
      "Epoch time taken:  7.260516405105591\n",
      "val Loss: 0.1710 Acc: 0.9227\n",
      "Epoch time taken:  8.334006547927856\n",
      "Epoch 48/59:LR: 1.0000000000000004e-08\n",
      "----------\n",
      "train Loss: 0.0507 Acc: 0.9813\n",
      "Epoch time taken:  7.002340316772461\n",
      "val Loss: 0.1844 Acc: 0.9227\n",
      "Epoch time taken:  8.035703182220459\n",
      "Epoch 49/59:LR: 1.0000000000000004e-08\n",
      "----------\n",
      "train Loss: 0.0634 Acc: 0.9756\n",
      "Epoch time taken:  7.045307159423828\n",
      "val Loss: 0.1976 Acc: 0.9175\n",
      "Epoch time taken:  8.074350595474243\n",
      "Epoch 50/59:LR: 1.0000000000000004e-08\n",
      "----------\n",
      "train Loss: 0.0696 Acc: 0.9700\n",
      "Epoch time taken:  7.013470888137817\n",
      "val Loss: 0.1775 Acc: 0.9278\n",
      "Epoch time taken:  8.052600622177124\n",
      "Epoch 51/59:LR: 1.0000000000000004e-08\n",
      "----------\n",
      "train Loss: 0.0617 Acc: 0.9731\n",
      "Epoch time taken:  7.080439805984497\n",
      "val Loss: 0.1809 Acc: 0.9227\n",
      "Epoch time taken:  8.107937812805176\n",
      "Epoch 52/59:LR: 1.0000000000000004e-08\n",
      "----------\n",
      "train Loss: 0.0523 Acc: 0.9794\n",
      "Epoch time taken:  7.048401594161987\n",
      "val Loss: 0.1789 Acc: 0.9227\n",
      "Epoch time taken:  8.07820725440979\n",
      "Epoch 53/59:LR: 1.0000000000000004e-08\n",
      "----------\n",
      "train Loss: 0.0617 Acc: 0.9713\n",
      "Epoch time taken:  7.127840280532837\n",
      "val Loss: 0.1781 Acc: 0.9253\n",
      "Epoch time taken:  8.154114007949829\n",
      "Epoch 54/59:LR: 1.0000000000000004e-08\n",
      "----------\n",
      "train Loss: 0.0563 Acc: 0.9813\n",
      "Epoch time taken:  7.0998804569244385\n",
      "val Loss: 0.1839 Acc: 0.9227\n",
      "Epoch time taken:  8.130563974380493\n",
      "Epoch 55/59:LR: 1.0000000000000004e-08\n",
      "----------\n",
      "train Loss: 0.0566 Acc: 0.9781\n",
      "Epoch time taken:  7.112043380737305\n",
      "val Loss: 0.1811 Acc: 0.9201\n",
      "Epoch time taken:  8.141807317733765\n",
      "Epoch 56/59:LR: 1.0000000000000004e-08\n",
      "----------\n",
      "train Loss: 0.0667 Acc: 0.9719\n",
      "Epoch time taken:  7.1376447677612305\n",
      "val Loss: 0.1962 Acc: 0.9201\n",
      "Epoch time taken:  8.178710460662842\n",
      "Epoch 57/59:LR: 1.0000000000000004e-08\n",
      "----------\n",
      "train Loss: 0.0671 Acc: 0.9763\n",
      "Epoch time taken:  7.064239263534546\n",
      "val Loss: 0.1943 Acc: 0.9149\n",
      "Epoch time taken:  8.101042985916138\n",
      "Epoch 58/59:LR: 1.0000000000000004e-08\n",
      "----------\n",
      "train Loss: 0.0492 Acc: 0.9850\n",
      "Epoch time taken:  7.0645692348480225\n",
      "val Loss: 0.1832 Acc: 0.9253\n",
      "Epoch time taken:  8.30558180809021\n",
      "Epoch 59/59:LR: 1.0000000000000004e-08\n",
      "----------\n",
      "train Loss: 0.0575 Acc: 0.9813\n",
      "Epoch time taken:  7.044184684753418\n",
      "val Loss: 0.1963 Acc: 0.9253\n",
      "Epoch time taken:  8.195795774459839\n",
      "Training complete in 8m 9s\n",
      "Best val Acc: 0.930412\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params_to_update = model.parameters()\n",
    "# Now we'll use Adam optimization\n",
    "optimizer = optim.Adam(params_to_update, lr=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',patience=5)\n",
    "t = trainer(device,criterion, optimizer,scheduler)\n",
    "model = t.train(model, dataloaders, num_epochs=60, weights_name='seresnet18_chihuahua_muffin_adam_0.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 16\n",
       "    Root location: data/test"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "test_dataset = torchvision.datasets.ImageFolder('data/test', transform=preprocess)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'test_dataloader' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-47485fa6eec9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "t.test(model,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}