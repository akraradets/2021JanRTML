{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Lab13 - st121413\n",
    "\n",
    "my DQN and memory"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import queue\n",
    "import numpy as np\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, buffer_size):\n",
    "        self.memory = queue.Queue()\n",
    "        self.buffer_size = buffer_size\n",
    "    def get_memory(self):\n",
    "        return list(self.memory.queue)\n",
    "\n",
    "    def get_memory_random(self):\n",
    "        index = np.random.randint(self.memory.qsize(), size=1)\n",
    "        return self.memory.queue[index[0]]\n",
    "\n",
    "    def add_memory(self, s_t, a_t, r_t, s_t1):\n",
    "        temp = (s_t, a_t, r_t, s_t1)\n",
    "        if(self.memory.qsize() > self.buffer_size):\n",
    "            self.memory.get()\n",
    "        self.memory.put(temp)\n",
    "        return True\n",
    "    def reset(self):\n",
    "        self.memory = queue.Queue()\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, number_action):\n",
    "        super(DQN, self).__init__()\n",
    "        # we would have just 27 inputs and 9 outputs. Two fully connected layers of 10 units each would give us 10x28+10x11+9x11=489 parameters\n",
    "        self.fc = nn.Linear(in_features=27, out_features=10)\n",
    "        self.fc2 = nn.Linear(in_features=10, out_features=10)\n",
    "        self.fc3 = nn.Linear(in_features=10, out_features=number_action)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # state = torch.tensor(state.reshape(-1).astype(float), requires_grad=True).float()\n",
    "        # print(state)\n",
    "        # # state = torch.from_numpy(state, requires_grad=True)\n",
    "        # # state = state.reshape(-1)\n",
    "        # state.requires_grad_(True)\n",
    "\n",
    "        # print(state.shape, type(state.float()),state)\n",
    "        out = self.fc(state)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "source": [
    "The modified q_learning function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, gamma, n_episode, alpha, player):\n",
    "    \"\"\"\n",
    "    Obtain the optimal policy with off-policy Q-learning method\n",
    "    @param env: OpenAI Gym environment\n",
    "    @param gamma: discount factor\n",
    "    @param n_episode: number of episodes\n",
    "    @return: the optimal Q-function, and the optimal policy\n",
    "    \"\"\"\n",
    "    n_action = 9\n",
    "    buffer = 3\n",
    "    memory = Memory(buffer)\n",
    "    model = DQN(number_action=n_action)\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    Q = defaultdict(lambda: torch.zeros(n_action))\n",
    "    for episode in range(n_episode):\n",
    "        ep_loss = 0\n",
    "        if episode % 10000 == 9999:\n",
    "            print(f\"{episode} has loss {ep_loss}\")\n",
    "            # print(\"episode: \", episode + 1)\n",
    "        state = env.reset()\n",
    "        memory.reset()\n",
    "        state_o = state\n",
    "        state = hash(tuple(state.reshape(-1)))\n",
    "        is_done = False\n",
    "        with torch.set_grad_enabled(True):\n",
    "            while not is_done:\n",
    "                optimizer.zero_grad()\n",
    "                if env.to_play() == player:\n",
    "                    available_action = env.legal_actions()\n",
    "                    action = epsilon_greedy_policy(state, Q, available_action)\n",
    "                    next_state, reward, is_done = env.step(action)\n",
    "                    next_state_o = next_state\n",
    "                    next_state = hash(tuple(next_state.reshape(-1)))\n",
    "                    td_delta = reward + gamma * torch.max(Q[next_state]) - Q[state][action]\n",
    "                    Q[state][action] += alpha * td_delta\n",
    "                else:\n",
    "                    action = env.expert_agent()\n",
    "                    next_state, reward, is_done = env.step(action)\n",
    "                    next_state_o = next_state\n",
    "                    next_state = hash(tuple(next_state.reshape(-1)))\n",
    "\n",
    "                    if is_done:\n",
    "                        reward = -reward\n",
    "                        td_delta = reward + gamma * torch.max(Q[next_state]) - Q[state][action]\n",
    "                        Q[state][action] += alpha * td_delta\n",
    "\n",
    "                length_episode[episode] += 1\n",
    "                total_reward_episode[episode] += reward\n",
    "                memory.add_memory(state_o, action, reward, next_state_o)\n",
    "                \n",
    "                e = memory.get_memory_random()\n",
    "                y = torch.as_tensor(e[2]).float()\n",
    "                if is_done == False:\n",
    "                    data = torch.as_tensor(e[3].reshape(-1).astype(float)).float()\n",
    "                    data.requires_grad = True\n",
    "                    actions = torch.nn.functional.softmax( model(data.to(device)), dim=0)\n",
    "                    # print(\"outputs:\", outputs.shape)\n",
    "                    # print(\"output:\", torch.argmax(outputs) )\n",
    "                    y = e[2] + gamma * Q[hash(tuple(e[3].reshape(-1)))][torch.argmax(actions)]\n",
    "                # print(\"y:\" , y)\n",
    "\n",
    "                # a = torch.argmax(torch.nn.functional.softmax(model(e[0]),dim=0))\n",
    "                # print(a.view(1,-1),\"=|||||=\",y)\n",
    "                y.requires_grad = True\n",
    "                # print(y.requires_grad)\n",
    "                loss = criterion(Q[hash(tuple(e[0].reshape(-1)))][e[1]], y.view(1,-1))\n",
    "                ep_loss += loss\n",
    "                # loss.requres_grad = True\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if(is_done):\n",
    "                    break\n",
    "                state = next_state\n",
    "        \n",
    "\n",
    "    policy = {}\n",
    "    for state, actions in Q.items():\n",
    "        policy[state] = torch.argmax(actions).item()\n",
    "    return Q, policy"
   ]
  },
  {
   "source": [
    "I tried but I think I messed up at calculating the y_i. Therefore, my loss is always 0 or 1. \n",
    "At this moment (5AM of Friday), I decided to submit what I have."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}