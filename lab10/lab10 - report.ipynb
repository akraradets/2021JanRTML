{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python369jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.6.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Lab 10 - st121413"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Change the structure to be identical to Goodfellow's Figure 10.3 with tanh activation functions and see if you get different results."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        # A bit more efficient than normal Softmax\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        a = self.i2h(combined)\n",
    "        hidden = torch.tanh(a)\n",
    "        o = self.h2o(hidden)\n",
    "        y_hat = self.softmax(o)\n",
    "        # hidden = self.i2h(combined)\n",
    "        # output = self.i2o(combined)\n",
    "        # output = self.softmax(output)\n",
    "        return y_hat, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "source": [
    "The result of Goodfellow likes RNN performe similar or a bit better at best.\n",
    "\n",
    "The given RNN loss\n",
    "\n",
    "![alt text](screenshots/inlab-loss.png)\n",
    "\n",
    "The Goodfellow likes RNN loss\n",
    "\n",
    "![alt text](screenshots/1-loss.png)\n",
    "\n",
    "The given RNN confusion martix\n",
    "\n",
    "![alt text](screenshots/inlab-matrix.png)\n",
    "\n",
    "The Goodfellow likes RNN confusion matrix\n",
    "\n",
    "![alt text](screenshots/1-matrix.png)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. Explore methods for batching patterns of different length prior to presentation to a RNN and implement them. See how much speedup you can get from the GPU with minibatch training.\n",
    "\n",
    "This one requires a bit of modification.\n",
    "\n",
    "First, I consult https://www.marktechpost.com/2020/04/12/implementing-batching-for-seq2seq-models-in-pytorch/ to see how to do batching.\n",
    "\n",
    "It turns out that our previous model takes tensor of shape (word_lenght, 1, characters) and that second dimension means the size of the batch.\n",
    "\n",
    "Idea is we want to increase the number of 1 but all the word must have the same lenght (tensor problem) so we need to pad the smaller word to equal to the biggest word in that batch."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def randomChoice(l):\n",
    "    # random.randint range is inclusive thus len(l)-1\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample(batch_size = 1):\n",
    "    if(batch_size == 1):\n",
    "        category = randomChoice(all_categories)\n",
    "        line = randomChoice(category_lines[category])\n",
    "        category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "        line_tensor = lineToTensor(line)\n",
    "        return category, line, category_tensor, line_tensor\n",
    "    else:\n",
    "        max_length = 0\n",
    "        categories = []\n",
    "        lines = []\n",
    "        lines_length = []\n",
    "        # Randomly choose words from our data \n",
    "        for i in range(batch_size):\n",
    "            category = randomChoice(all_categories)\n",
    "            line = randomChoice(category_lines[category])\n",
    "            categories.append(category)\n",
    "            lines.append(line)\n",
    "            lines_length.append(len(line))\n",
    "            # If our random word_i has the greatest lenght, save that number\n",
    "            if(len(line) > max_length): max_length = len(line)\n",
    "        # padding function\n",
    "        line_tensor = batched_lines(lines,max_length)\n",
    "        # just pack all the tags in to tensor form\n",
    "        category_tensor = batched_categories(categories)\n",
    "        return categories, lines, category_tensor, line_tensor"
   ]
  },
  {
   "source": [
    "batched_lines function will pad all the word with all zero array after each word until the size of that word is equal to the biggest size in the list. Then pack those words into tensor.\n",
    "\n",
    "batched_categories function is transform tags into tensor."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.marktechpost.com/2020/04/12/implementing-batching-for-seq2seq-models-in-pytorch/\n",
    "def batched_lines(names, max_word_size):\n",
    "    rep = torch.zeros(max_word_size, len(names), n_letters)\n",
    "    for name_index, name in enumerate(names):\n",
    "        for letter_index, letter in enumerate(name):\n",
    "            pos = all_letters.find(letter)\n",
    "            rep[letter_index][name_index][pos] = 1\n",
    "    return rep\n",
    "\n",
    "def batched_categories(langs):\n",
    "    rep = torch.zeros([len(langs)], dtype=torch.long)\n",
    "    for index, lang in enumerate(langs):\n",
    "        rep[index] = all_categories.index(lang)\n",
    "    return rep"
   ]
  },
  {
   "source": [
    "Lastly, the initeHidden must be able to generate hidden of (batch_size, hidden_size). "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        # A bit more efficient than normal Softmax\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # print(input.shape, hidden.shape)\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        a = self.i2h(combined)\n",
    "        hidden = torch.tanh(a)\n",
    "        o = self.h2o(hidden)\n",
    "        y_hat = self.softmax(o)\n",
    "        # hidden = self.i2h(combined)\n",
    "        # output = self.i2o(combined)\n",
    "        # output = self.softmax(output)\n",
    "        return y_hat, hidden\n",
    "\n",
    "    def initHidden(self, batch_size = 1):\n",
    "        return torch.zeros(batch_size, self.hidden_size)"
   ]
  },
  {
   "source": [
    "The result.\n",
    "\n",
    "When train at batch_size = 1, the model trains slower. I think it is due to inefficient of moving data in and out of the GPU.\n",
    "\n",
    "When train at batch_size > 1.\n",
    "- number of iter = 100000/batch_size: with this condition, the model sees the same number of words. It trains faster but perform noticibly worst.\n",
    "- number of iter = 100000: with this condition, the model sees more words (order of batch_size). It trains slower but perform close to the previous non-btach model\n",
    "\n",
    "\n",
    "The graph is for iter = 100000 and batch size of 10\n",
    "\n",
    "The loss\n",
    "\n",
    "![alt text](screenshots/2-loss.png)\n",
    "\n",
    "The confusion matrix\n",
    "\n",
    "![alt text](screenshots/2-matrix.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3. Do a bit of research on similar problems such as named entity recognition, find a dataset, train a model, and report your results.\n",
    "\n",
    "I use the dataset from here becuase I want to perform NER.\n",
    "\n",
    "https://towardsdatascience.com/named-entity-recognition-ner-using-keras-bidirectional-lstm-28cd3f301f54\n",
    "\n",
    "\n",
    "Later on, I found out that the source train the model differently from what we have done so far. Therefore, I change my mind to predict POS based on word.\n",
    "\n",
    "\n",
    "I change the preparation section to prepare the dataset. This way, we predict a POS from a sequence of characters. (same idea of predict category based on a sequence)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "for pos in list(set(data['POS'].to_list())):\n",
    "    category_lines[pos] = []\n",
    "    all_categories.append(pos)\n",
    "\n",
    "for word,pos in zip(data['Word'].to_list(),data['POS'].to_list()):\n",
    "    category_lines[pos].append(word)"
   ]
  },
  {
   "source": [
    "The loss\n",
    "\n",
    "![alt text](screenshots/3-loss.png)\n",
    "\n",
    "the confusion matrix\n",
    "\n",
    "![alt text](screenshots/3-matrix.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}