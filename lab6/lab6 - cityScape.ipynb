{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ln -s ~/Cityscapes/ ./Cityscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import datasets\n",
    "# # https://pytorch.org/vision/0.8/datasets.html#cityscapes\n",
    "# # split (string, optional) – The image split to use, train, test or val if mode=”fine” otherwise train, train_extra or val\n",
    "# # dataset = datasets.Cityscapes('./data/cityscapes', split='train', mode='fine', target_type='semantic')\n",
    "# target_type = ['instance', 'semantic', 'polygon', 'color']\n",
    "# dataset = datasets.Cityscapes('./Cityscapes', split='val', mode='fine', target_type=target_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading pretrained model...\n",
      "loading annotations into memory...\n",
      "Done (t=0.63s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.datasets import CocoDetection\n",
    "\n",
    "import utils\n",
    "from coco_utils import get_city\n",
    "import transforms\n",
    "\n",
    "# Load a model pre-trained on COCO and put it in inference mode\n",
    "\n",
    "print('Loading pretrained model...')\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Load the COCO 2017 train and val sets. We use the CocoDetection class definition\n",
    "# from ./coco_utils.py, not the original torchvision.CocoDetection class. Also, we\n",
    "# use transforms from ./transforms, not torchvision.transforms, because they need\n",
    "# to transform the bboxes and masks along with the image.\n",
    "\n",
    "# coco_path = \"./COCO\"\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "# print('Loading COCO train, val datasets...')\n",
    "coco_val_dataset = get_city(preprocess)\n",
    "# coco_val_dataset = get_coco(coco_path, 'val', transform)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(coco_val_dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(val_dataloader))\n",
    "\n",
    "# print(type(images), len(images))\n",
    "# print(images[0].shape)\n",
    "# print(type(targets), len(targets))\n",
    "# print(targets[0].keys())\n",
    "# # print()\n",
    "# # images = images.to(device)\n",
    "\n",
    "images = [ img.cuda() for img in images ]\n",
    "images.to(device)\n",
    "predictions = model(images)\n",
    "\n",
    "print('Prediction keys:', list(dict(predictions[0])))\n",
    "print('Boxes shape:', predictions[0]['boxes'].shape)\n",
    "print('Labels shape:', predictions[0]['labels'].shape)\n",
    "print('Scores shape:', predictions[0]['scores'].shape)\n",
    "print('Masks shape:', predictions[0]['masks'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(val_dataloader))\n",
    "\n",
    "print(type(images), len(images))\n",
    "print(images[0].shape)\n",
    "print(type(targets), len(targets))\n",
    "print(targets[0][2])\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# plt.figure(1, figsize=(12, 9), dpi=100)\n",
    "# plt.imshow(targets[0][0])\n",
    "# plt.title('Validation image result')\n",
    "# plt.show()\n",
    "\n",
    "# images = images.to(device)\n",
    "\n",
    "# images = [ img.to(device) for img in images ]\n",
    "# predictions = model(images)\n",
    "\n",
    "# print('Prediction keys:', list(dict(predictions[0])))\n",
    "# print('Boxes shape:', predictions[0]['boxes'].shape)\n",
    "# print('Labels shape:', predictions[0]['labels'].shape)\n",
    "# print('Scores shape:', predictions[0]['scores'].shape)\n",
    "# print('Masks shape:', predictions[0]['masks'].shape)"
   ]
  }
 ]
}