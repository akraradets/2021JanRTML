{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Lab14 - st121413\n",
    "\n",
    "Independent exercise\n",
    "Recall that Mnih et al. (2015) obtained superhuman playing ability for some of the Atari games by combining DQN with experience replay and a more sophisticated state representation that what we've seen so far: they stack successive frames as the input state representation so as to give the agent some \"velocity\" input rather than a static snapshot of the scene.\n",
    "\n",
    "Try combining the techniques we've developed in the lab with the frame history as state, and get the best Space Invaders player you can. What's your agent's average and best score over 100 games?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "I continue from Prioritized experience replay. From there, I notice that to get the stack of observation, I only need to modify the <code>get_state2</code> a bit.\n",
    "\n",
    "First, I transform the observation to 1-channel grayscale. Then, I stack them up in a shape of (queue,width,height). The queue is max to length of 3 and init with all the first frame. Later on, I pop one out and put new observation in.\n",
    "\n",
    "This way, I can just put all 3 observations as an input without modifying the DDQN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 84\n",
    "transform = T.Compose([T.ToPILImage(),\n",
    "                       T.Grayscale(num_output_channels=1),\n",
    "                       T.Resize((image_size, image_size), interpolation=Image.CUBIC),\n",
    "                       T.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "state_buffer = queue.Queue()\n",
    "def get_state3(observation):\n",
    "    \n",
    "    # First time, repeat the state for 3 times\n",
    "    if(state_buffer.qsize() == 0):\n",
    "        for i in range(3):\n",
    "            state = get_state2(observation)\n",
    "            state_buffer.put(state)\n",
    "        # print(observation.shape, state.shape)\n",
    "    else:\n",
    "        state_buffer.get()\n",
    "        state = get_state2(observation)\n",
    "        state_buffer.put(state)\n",
    "    # for i in state_buffer.queue:\n",
    "    #     print(i.shape)\n",
    "    rep = torch.cat(list(state_buffer.queue), dim=0)\n",
    "    # print(\"rep=====\",rep.shape)\n",
    "    return rep\n",
    "\n",
    "def get_state2(observation):\n",
    "    state = observation.transpose((2,0,1))\n",
    "    state = torch.from_numpy(state)\n",
    "    state = transform(state)\n",
    "    return state"
   ]
  },
  {
   "source": [
    "## result\n",
    "First, I trained the modal for 200,000 epochs and found that the performance is not any better than what we have done so far.\n",
    "- Mainly, the agent only shooting at the start spot. It got 215 at best.\n",
    "\n",
    "I suspected the number of epoches is to little so I train for 1,000,000 epochs and found a little improvement.\n",
    "- The agent move the ship to the right and shooting from there. Now, it got 285.\n",
    "\n",
    "Then, I trained more to 50,000,000 epoches.\n",
    "- The agent act the same as the 1M one. But, it able to shoot the purple UFO and got 800 scores at the end.\n",
    "- Unfortunately, without the UFO, the agent got 285 score. The only different from the 1M agent is it wiggles (move slightly left and right) when the laser is about to hit the spaceship."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}