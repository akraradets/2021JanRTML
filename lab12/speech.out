COMET INFO: Experiment is live on comet.ml https://www.comet.ml/akraradets/2021janrtml-lab12/b34e0f3c19a846a9b4aa05acf5cc2142

/usr/local/lib/python3.8/dist-packages/torchaudio/functional/functional.py:357: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.
  warnings.warn(
SpeechRecognitionModel(
  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (rescnn_layers): Sequential(
    (0): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (layer_norm1): CNNLayerNorm(
        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm2): CNNLayerNorm(
        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (layer_norm1): CNNLayerNorm(
        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm2): CNNLayerNorm(
        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): ResidualCNN(
      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (layer_norm1): CNNLayerNorm(
        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm2): CNNLayerNorm(
        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (fully_connected): Linear(in_features=2048, out_features=512, bias=True)
  (birnn_layers): Sequential(
    (0): BidirectionalGRU(
      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (3): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (4): BidirectionalGRU(
      (BiGRU): GRU(1024, 512, bidirectional=True)
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): GELU()
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=512, out_features=29, bias=True)
  )
)
Num Model Parameters 23705373
/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:1289: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
Train Epoch: 1 [0/28539 (0%)]	Loss: 7.014989
Train Epoch: 1 [2000/28539 (7%)]	Loss: 2.910947
Train Epoch: 1 [4000/28539 (14%)]	Loss: 2.863532
Train Epoch: 1 [6000/28539 (21%)]	Loss: 2.869100
Train Epoch: 1 [8000/28539 (28%)]	Loss: 2.864099
Train Epoch: 1 [10000/28539 (35%)]	Loss: 2.840993
Train Epoch: 1 [12000/28539 (42%)]	Loss: 2.860250
Train Epoch: 1 [14000/28539 (49%)]	Loss: 2.851711
Train Epoch: 1 [16000/28539 (56%)]	Loss: 2.884032
Train Epoch: 1 [18000/28539 (63%)]	Loss: 2.841996
Train Epoch: 1 [20000/28539 (70%)]	Loss: 2.803820
Train Epoch: 1 [22000/28539 (77%)]	Loss: 2.612340
Train Epoch: 1 [24000/28539 (84%)]	Loss: 2.240766
Train Epoch: 1 [26000/28539 (91%)]	Loss: 2.158986
Train Epoch: 1 [28000/28539 (98%)]	Loss: 1.907002

evaluating...
Test set: Average loss: 1.7637, Average CER: 0.512827 Average WER: 1.0584

Train Epoch: 2 [0/28539 (0%)]	Loss: 1.999731
Train Epoch: 2 [2000/28539 (7%)]	Loss: 1.807507
Train Epoch: 2 [4000/28539 (14%)]	Loss: 1.709452
Train Epoch: 2 [6000/28539 (21%)]	Loss: 1.676901
Train Epoch: 2 [8000/28539 (28%)]	Loss: 1.709979
Train Epoch: 2 [10000/28539 (35%)]	Loss: 1.571398
Train Epoch: 2 [12000/28539 (42%)]	Loss: 1.498631
Train Epoch: 2 [14000/28539 (49%)]	Loss: 1.468657
Train Epoch: 2 [16000/28539 (56%)]	Loss: 1.535809
Train Epoch: 2 [18000/28539 (63%)]	Loss: 1.434660
Train Epoch: 2 [20000/28539 (70%)]	Loss: 1.399793
Train Epoch: 2 [22000/28539 (77%)]	Loss: 1.432506
Train Epoch: 2 [24000/28539 (84%)]	Loss: 1.383432
Train Epoch: 2 [26000/28539 (91%)]	Loss: 1.412840
Train Epoch: 2 [28000/28539 (98%)]	Loss: 1.253503

evaluating...
Test set: Average loss: 1.1004, Average CER: 0.337160 Average WER: 0.8150

Train Epoch: 3 [0/28539 (0%)]	Loss: 1.263575
Train Epoch: 3 [2000/28539 (7%)]	Loss: 1.249066
Train Epoch: 3 [4000/28539 (14%)]	Loss: 1.222758
Train Epoch: 3 [6000/28539 (21%)]	Loss: 1.371051
Train Epoch: 3 [8000/28539 (28%)]	Loss: 1.161581
Train Epoch: 3 [10000/28539 (35%)]	Loss: 1.240332
Train Epoch: 3 [12000/28539 (42%)]	Loss: 1.227030
Train Epoch: 3 [14000/28539 (49%)]	Loss: 1.078622
Train Epoch: 3 [16000/28539 (56%)]	Loss: 1.123914
Train Epoch: 3 [18000/28539 (63%)]	Loss: 1.147645
Train Epoch: 3 [20000/28539 (70%)]	Loss: 1.149091
Train Epoch: 3 [22000/28539 (77%)]	Loss: 1.103633
Train Epoch: 3 [24000/28539 (84%)]	Loss: 1.110492
Train Epoch: 3 [26000/28539 (91%)]	Loss: 1.244101
Train Epoch: 3 [28000/28539 (98%)]	Loss: 1.210908

evaluating...
Test set: Average loss: 0.9082, Average CER: 0.278190 Average WER: 0.7446

Train Epoch: 4 [0/28539 (0%)]	Loss: 1.095999
Train Epoch: 4 [2000/28539 (7%)]	Loss: 1.225736
Train Epoch: 4 [4000/28539 (14%)]	Loss: 1.033165
Train Epoch: 4 [6000/28539 (21%)]	Loss: 1.061673
Train Epoch: 4 [8000/28539 (28%)]	Loss: 1.068554
Train Epoch: 4 [10000/28539 (35%)]	Loss: 1.078643
Train Epoch: 4 [12000/28539 (42%)]	Loss: 1.021406
Train Epoch: 4 [14000/28539 (49%)]	Loss: 0.997142
Train Epoch: 4 [16000/28539 (56%)]	Loss: 0.987662
Train Epoch: 4 [18000/28539 (63%)]	Loss: 0.925182
Train Epoch: 4 [20000/28539 (70%)]	Loss: 1.058820
Train Epoch: 4 [22000/28539 (77%)]	Loss: 0.958047
Train Epoch: 4 [24000/28539 (84%)]	Loss: 0.983614
Train Epoch: 4 [26000/28539 (91%)]	Loss: 0.980205
Train Epoch: 4 [28000/28539 (98%)]	Loss: 0.818430

evaluating...
Test set: Average loss: 0.7995, Average CER: 0.243695 Average WER: 0.6864

Train Epoch: 5 [0/28539 (0%)]	Loss: 0.866176
Train Epoch: 5 [2000/28539 (7%)]	Loss: 1.107819
Train Epoch: 5 [4000/28539 (14%)]	Loss: 0.938618
Train Epoch: 5 [6000/28539 (21%)]	Loss: 0.902341
Train Epoch: 5 [8000/28539 (28%)]	Loss: 0.780303
Train Epoch: 5 [10000/28539 (35%)]	Loss: 0.940328
Train Epoch: 5 [12000/28539 (42%)]	Loss: 0.863793
Train Epoch: 5 [14000/28539 (49%)]	Loss: 0.930846
Train Epoch: 5 [16000/28539 (56%)]	Loss: 0.941076
Train Epoch: 5 [18000/28539 (63%)]	Loss: 0.904253
Train Epoch: 5 [20000/28539 (70%)]	Loss: 0.965758
Train Epoch: 5 [22000/28539 (77%)]	Loss: 0.842829
Train Epoch: 5 [24000/28539 (84%)]	Loss: 0.923892
Train Epoch: 5 [26000/28539 (91%)]	Loss: 0.872715
Train Epoch: 5 [28000/28539 (98%)]	Loss: 0.885781

evaluating...
Test set: Average loss: 0.6917, Average CER: 0.214425 Average WER: 0.6038

Train Epoch: 6 [0/28539 (0%)]	Loss: 0.868920
Train Epoch: 6 [2000/28539 (7%)]	Loss: 0.860440
Train Epoch: 6 [4000/28539 (14%)]	Loss: 0.970870
Train Epoch: 6 [6000/28539 (21%)]	Loss: 0.804913
Train Epoch: 6 [8000/28539 (28%)]	Loss: 0.871382
Train Epoch: 6 [10000/28539 (35%)]	Loss: 1.083471
Train Epoch: 6 [12000/28539 (42%)]	Loss: 0.915944
Train Epoch: 6 [14000/28539 (49%)]	Loss: 0.770670
Train Epoch: 6 [16000/28539 (56%)]	Loss: 0.811834
Train Epoch: 6 [18000/28539 (63%)]	Loss: 0.815828
Train Epoch: 6 [20000/28539 (70%)]	Loss: 0.942099
Train Epoch: 6 [22000/28539 (77%)]	Loss: 0.879465
Train Epoch: 6 [24000/28539 (84%)]	Loss: 0.926801
Train Epoch: 6 [26000/28539 (91%)]	Loss: 0.920456
Train Epoch: 6 [28000/28539 (98%)]	Loss: 0.792801

evaluating...
Test set: Average loss: 0.6471, Average CER: 0.206673 Average WER: 0.5681

Train Epoch: 7 [0/28539 (0%)]	Loss: 0.707318
Train Epoch: 7 [2000/28539 (7%)]	Loss: 0.806893
Train Epoch: 7 [4000/28539 (14%)]	Loss: 0.784512
Train Epoch: 7 [6000/28539 (21%)]	Loss: 0.792909
Train Epoch: 7 [8000/28539 (28%)]	Loss: 0.820969
Train Epoch: 7 [10000/28539 (35%)]	Loss: 0.838563
Train Epoch: 7 [12000/28539 (42%)]	Loss: 0.739127
Train Epoch: 7 [14000/28539 (49%)]	Loss: 0.778562
Train Epoch: 7 [16000/28539 (56%)]	Loss: 0.854984
Train Epoch: 7 [18000/28539 (63%)]	Loss: 0.699160
Train Epoch: 7 [20000/28539 (70%)]	Loss: 0.683561
Train Epoch: 7 [22000/28539 (77%)]	Loss: 0.800421
Train Epoch: 7 [24000/28539 (84%)]	Loss: 0.790050
Train Epoch: 7 [26000/28539 (91%)]	Loss: 0.772408
Train Epoch: 7 [28000/28539 (98%)]	Loss: 0.667991

evaluating...
Test set: Average loss: 0.6026, Average CER: 0.183044 Average WER: 0.5326

Train Epoch: 8 [0/28539 (0%)]	Loss: 0.813577
Train Epoch: 8 [2000/28539 (7%)]	Loss: 0.697434
Train Epoch: 8 [4000/28539 (14%)]	Loss: 0.663812
Train Epoch: 8 [6000/28539 (21%)]	Loss: 0.800108
Train Epoch: 8 [8000/28539 (28%)]	Loss: 0.728511
Train Epoch: 8 [10000/28539 (35%)]	Loss: 0.708714
Train Epoch: 8 [12000/28539 (42%)]	Loss: 0.730903
Train Epoch: 8 [14000/28539 (49%)]	Loss: 0.644782
Train Epoch: 8 [16000/28539 (56%)]	Loss: 0.753016
Train Epoch: 8 [18000/28539 (63%)]	Loss: 0.758803
Train Epoch: 8 [20000/28539 (70%)]	Loss: 0.859905
Train Epoch: 8 [22000/28539 (77%)]	Loss: 0.797156
Train Epoch: 8 [24000/28539 (84%)]	Loss: 0.719850
Train Epoch: 8 [26000/28539 (91%)]	Loss: 0.752595
Train Epoch: 8 [28000/28539 (98%)]	Loss: 0.740082

evaluating...
Test set: Average loss: 0.5737, Average CER: 0.174013 Average WER: 0.5124

Train Epoch: 9 [0/28539 (0%)]	Loss: 0.726439
Train Epoch: 9 [2000/28539 (7%)]	Loss: 0.631145
Train Epoch: 9 [4000/28539 (14%)]	Loss: 0.709415
Train Epoch: 9 [6000/28539 (21%)]	Loss: 0.763784
Train Epoch: 9 [8000/28539 (28%)]	Loss: 0.620679
Train Epoch: 9 [10000/28539 (35%)]	Loss: 0.664681
Train Epoch: 9 [12000/28539 (42%)]	Loss: 0.617230
Train Epoch: 9 [14000/28539 (49%)]	Loss: 0.656353
Train Epoch: 9 [16000/28539 (56%)]	Loss: 0.702953
Train Epoch: 9 [18000/28539 (63%)]	Loss: 0.731390
Train Epoch: 9 [20000/28539 (70%)]	Loss: 0.642114
Train Epoch: 9 [22000/28539 (77%)]	Loss: 0.674883
Train Epoch: 9 [24000/28539 (84%)]	Loss: 0.707617
Train Epoch: 9 [26000/28539 (91%)]	Loss: 0.728269
Train Epoch: 9 [28000/28539 (98%)]	Loss: 0.768598

evaluating...
Test set: Average loss: 0.5514, Average CER: 0.167036 Average WER: 0.4928

Train Epoch: 10 [0/28539 (0%)]	Loss: 0.674705
Train Epoch: 10 [2000/28539 (7%)]	Loss: 0.697320
Train Epoch: 10 [4000/28539 (14%)]	Loss: 0.681419
Train Epoch: 10 [6000/28539 (21%)]	Loss: 0.613643
Train Epoch: 10 [8000/28539 (28%)]	Loss: 0.678247
Train Epoch: 10 [10000/28539 (35%)]	Loss: 0.596522
Train Epoch: 10 [12000/28539 (42%)]	Loss: 0.670394
Train Epoch: 10 [14000/28539 (49%)]	Loss: 0.617777
Train Epoch: 10 [16000/28539 (56%)]	Loss: 0.704951
Train Epoch: 10 [18000/28539 (63%)]	Loss: 0.642689
Train Epoch: 10 [20000/28539 (70%)]	Loss: 0.662165
Train Epoch: 10 [22000/28539 (77%)]	Loss: 0.712787
Train Epoch: 10 [24000/28539 (84%)]	Loss: 0.699666
Train Epoch: 10 [26000/28539 (91%)]	Loss: 0.787624
Train Epoch: 10 [28000/28539 (98%)]	Loss: 0.603533

evaluating...
Test set: Average loss: 0.5392, Average CER: 0.161768 Average WER: 0.4812

COMET INFO: ---------------------------
COMET INFO: Comet.ml Experiment Summary
COMET INFO: ---------------------------
COMET INFO:   Data:
COMET INFO:     display_summary_level : 1
COMET INFO:     url                   : https://www.comet.ml/akraradets/2021janrtml-lab12/b34e0f3c19a846a9b4aa05acf5cc2142
COMET INFO:   Metrics [count] (min, max):
COMET INFO:     cer [10]                    : (0.16176752009619932, 0.5128271411538571)
COMET INFO:     test_loss [10]              : (0.5392132128467996, 1.7636986306605449)
COMET INFO:     train_learning_rate [14270] : (2.0000000000314905e-09, 0.0005)
COMET INFO:     train_loss [15698]          : (0.5110462307929993, 7.014988899230957)
COMET INFO:     wer [10]                    : (0.4811787704943892, 1.0584317845898943)
COMET INFO:   Others:
COMET INFO:     Name : deepspeech
COMET INFO:   Uploads:
COMET INFO:     environment details      : 1
COMET INFO:     filename                 : 1
COMET INFO:     git metadata             : 1
COMET INFO:     git-patch (uncompressed) : 1 (739 bytes)
COMET INFO:     installed packages       : 1
COMET INFO:     model graph              : 1
COMET INFO:     os packages              : 1
COMET INFO:     source_code              : 1
COMET INFO: ---------------------------
COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)
COMET INFO: The Python SDK has 3600 seconds to finish before aborting...
