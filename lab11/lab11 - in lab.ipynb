{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python369jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.6.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class NaiveCustomLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_sz: int, hidden_sz: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        \n",
    "        # Parameters for computing i_t\n",
    "        self.U_i = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_i = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        # Parameters for computing f_t\n",
    "        self.U_f = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_f = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        # Parameters for computing c_t\n",
    "        self.U_c = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_c = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_c = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        # Parameters for computing o_t\n",
    "        self.U_o = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_o = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        self.init_weights()\n",
    "                \n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "         \n",
    "    def forward(self, x, init_states=None):\n",
    "        \"\"\"\n",
    "        forward: Run input x through the cell. Assumes x.shape is (batch_size, sequence_length, input_size)\n",
    "        \"\"\"\n",
    "        bs, seq_sz, _ = x.size()\n",
    "        hidden_seq = []\n",
    "        \n",
    "        if init_states is None:\n",
    "            h_t, c_t = (\n",
    "                torch.zeros(bs, self.hidden_size).to(x.device),\n",
    "                torch.zeros(bs, self.hidden_size).to(x.device),\n",
    "            )\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "            \n",
    "        for t in range(seq_sz):\n",
    "            x_t = x[:, t, :]\n",
    "            \n",
    "            i_t = torch.sigmoid(x_t @ self.U_i + h_t @ self.V_i + self.b_i)\n",
    "            f_t = torch.sigmoid(x_t @ self.U_f + h_t @ self.V_f + self.b_f)\n",
    "            g_t = torch.tanh(x_t @ self.U_c + h_t @ self.V_c + self.b_c)\n",
    "            o_t = torch.sigmoid(x_t @ self.U_o + h_t @ self.V_o + self.b_o)\n",
    "            c_t = f_t * c_t + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            \n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "        \n",
    "        # Reshape hidden_seq tensor to (batch size, sequence length, hidden_size)\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "\n",
    "        return hidden_seq, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserved word tokens\n",
    "\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "class Voc:\n",
    "\n",
    "    def __init__(self):        \n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10  # Maximum sentence length to consider\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Read query/response pairs and return a Voc object\n",
    "\n",
    "def readVocs(datafile):\n",
    "    print(\"Reading lines...\")    \n",
    "    # Read the file and split into lines\n",
    "    lines = open(datafile, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    voc = Voc()\n",
    "    return voc, pairs\n",
    "\n",
    "# Boolean function returning True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
    "\n",
    "def filterPair(p):\n",
    "    # Input sequences need to preserve the last word for EOS token\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "# Filter pairs using the filterPair predicate\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "# Using the functions defined above, return a populated voc object and pairs list\n",
    "\n",
    "def loadPrepareData(datafile):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    voc, pairs = readVocs(datafile)\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2021-04-02 09:25:24--  https://github.com/dsai-asia/RTML/raw/main/Labs/11-LSTMs/chatDataset.txt\n",
      "Connecting to 192.41.170.23:3128... connected.\n",
      "Proxy request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/dsai-asia/RTML/main/Labs/11-LSTMs/chatDataset.txt [following]\n",
      "--2021-04-02 09:25:24--  https://raw.githubusercontent.com/dsai-asia/RTML/main/Labs/11-LSTMs/chatDataset.txt\n",
      "Connecting to 192.41.170.23:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 24659640 (24M) [text/plain]\n",
      "Saving to: ‘chatDataset.txt’\n",
      "\n",
      "chatDataset.txt     100%[===================>]  23.52M  9.26MB/s    in 2.5s    \n",
      "\n",
      "2021-04-02 09:25:27 (9.26 MB/s) - ‘chatDataset.txt’ saved [24659640/24659640]\n",
      "\n",
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 221282 sentence pairs\n",
      "Trimmed to 64271 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 18008\n",
      "\n",
      "pairs:\n",
      "['there .', 'where ?']\n",
      "['you have my word . as a gentleman', 'you re sweet .']\n",
      "['hi .', 'looks like things worked out tonight huh ?']\n",
      "['you know chastity ?', 'i believe we share an art instructor']\n",
      "['have fun tonight ?', 'tons']\n",
      "['well no . . .', 'then that s all you had to say .']\n",
      "['then that s all you had to say .', 'but']\n",
      "['but', 'you always been this selfish ?']\n",
      "['do you listen to this crap ?', 'what crap ?']\n",
      "['what good stuff ?', 'the real you .']\n"
     ]
    }
   ],
   "source": [
    "# !wget https://github.com/dsai-asia/RTML/raw/main/Labs/11-LSTMs/chatDataset.txt\n",
    "\n",
    "# Load/Assemble Voc and pairs\n",
    "\n",
    "datafile = 'chatDataset.txt'\n",
    "voc, pairs = loadPrepareData(datafile)\n",
    "\n",
    "# Print some pairs to validate\n",
    "\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "keep_words 7823 / 18005 = 0.4345\nTrimmed from 64271 pairs to 53165, 0.8272 of total\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 3    # Minimum word count threshold for trimming\n",
    "\n",
    "def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "    # Trim words used under the MIN_COUNT from the voc\n",
    "    voc.trim(MIN_COUNT)\n",
    "    # Filter out pairs with trimmed words\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # Check input sentence\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # Check output sentence\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "    return keep_pairs\n",
    "\n",
    "# Trim vocabulary and pairs\n",
    "\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpairs = pairs[45000:]\n",
    "pairs  = pairs[:45000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# Return a padded input sequence tensor and the lengths of each original sequence\n",
    "\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "# Return a padded target sequence tensor, a padding mask, and the max target length\n",
    "\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "# Return all items for a given batch of pairs\n",
    "\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "# Example for validation\n",
    "\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['there .', 'where ?'], ['you have my word . as a gentleman', 'you re sweet .'], ['hi .', 'looks like things worked out tonight huh ?'], ['have fun tonight ?', 'tons'], ['well no . . .', 'then that s all you had to say .']]\n[['you have my word . as a gentleman', 'you re sweet .'], ['well no . . .', 'then that s all you had to say .'], ['have fun tonight ?', 'tons'], ['there .', 'where ?'], ['hi .', 'looks like things worked out tonight huh ?']]\ntensor([[ 100,   65,   27,   76,  147],\n        [  80,  112,   94,   37,   92],\n        [  53,  882,  117,  641,   19],\n        [6954,  247,    4,    6,  212],\n        [   2,  117,    4,    2,   21],\n        [   0,   65,    4,    0,    3],\n        [   0,    6,    2,    0,    6],\n        [   0,    2,    0,    0,    2]])\ntensor([[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [0, 1, 1, 0, 1],\n        [0, 1, 1, 0, 1],\n        [0, 1, 0, 0, 1]], dtype=torch.uint8)\n8\n"
     ]
    }
   ],
   "source": [
    "pair_batch = pairs[:5]\n",
    "print(pair_batch)\n",
    "pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "print(pair_batch)\n",
    "print(target_variable)\n",
    "print(mask)\n",
    "print(max_target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.LSTM(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        \n",
    "        embedded = self.embedding(input_seq)\n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths.cpu())\n",
    "        \n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        \n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        \n",
    "        return outputs, hidden\n",
    "    def init_hidden(self):\n",
    "        \n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "        attn_energies = attn_energies.t()\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        \n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "       \n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))#, bidirectional=True)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "    \n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "  \n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    \n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    \n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    \n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    \n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            \n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            \n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            \n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            \n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    \n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    losslist = []\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        \n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        \n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        \n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "            losslist.append(print_loss_avg)\n",
    "        \n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            print(directory)\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n",
    "    return losslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        \n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        \n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        \n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "       \n",
    "        for _ in range(max_length):\n",
    "            \n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            \n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        \n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "  \n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    \n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    \n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    \n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    \n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    \n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            \n",
    "            input_sentence = input('> ')\n",
    "            \n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            \n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            \n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            \n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "\n",
    "hidden_size = 512\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.5\n",
    "batch_size = 256 \n",
    "loadFilename = None\n",
    "\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ons.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "Iteration: 6000; Percent complete: 100.0%; Average loss: 1.7514\n",
      "content/cb_model/Chat/2-4_512\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'content/'\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 6000\n",
    "print_every = 10\n",
    "save_every = 2000\n",
    "loadFilename = None\n",
    "corpus_name=\"Chat\"\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "print(\"Starting Training!\")\n",
    "lossvalues = trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 362.5625 248.518125\" width=\"362.5625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-02T09:37:42.421888</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 362.5625 248.518125 \nL 362.5625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 20.5625 224.64 \nL 355.3625 224.64 \nL 355.3625 7.2 \nL 20.5625 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m0905f45cc2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.780682\" xlink:href=\"#m0905f45cc2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(32.599432 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"86.592641\" xlink:href=\"#m0905f45cc2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(77.048891 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"137.4046\" xlink:href=\"#m0905f45cc2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(127.86085 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"188.21656\" xlink:href=\"#m0905f45cc2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(178.67281 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"239.028519\" xlink:href=\"#m0905f45cc2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(229.484769 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"289.840478\" xlink:href=\"#m0905f45cc2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(280.296728 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"340.652438\" xlink:href=\"#m0905f45cc2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 600 -->\n      <g transform=\"translate(331.108688 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"med43f671ad\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#med43f671ad\" y=\"205.840307\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 2 -->\n      <g transform=\"translate(7.2 209.639526)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#med43f671ad\" y=\"175.61267\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 3 -->\n      <g transform=\"translate(7.2 179.411889)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#med43f671ad\" y=\"145.385033\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 4 -->\n      <g transform=\"translate(7.2 149.184252)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#med43f671ad\" y=\"115.157397\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 5 -->\n      <g transform=\"translate(7.2 118.956615)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#med43f671ad\" y=\"84.92976\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 6 -->\n      <g transform=\"translate(7.2 88.728979)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#med43f671ad\" y=\"54.702123\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 7 -->\n      <g transform=\"translate(7.2 58.501342)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#med43f671ad\" y=\"24.474487\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 8 -->\n      <g transform=\"translate(7.2 28.273705)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#pe65f65a04e)\" d=\"M 35.780682 17.083636 \nL 36.288801 89.404053 \nL 36.796921 113.831467 \nL 37.305041 114.495546 \nL 37.81316 118.76025 \nL 38.32128 119.594894 \nL 38.829399 120.212128 \nL 39.337519 121.554809 \nL 39.845639 122.097934 \nL 40.353758 121.573595 \nL 40.861878 123.142986 \nL 41.369997 122.657869 \nL 42.386237 126.783312 \nL 42.894356 131.131318 \nL 43.402476 131.430113 \nL 43.910595 130.521328 \nL 44.418715 132.508967 \nL 44.926834 133.047465 \nL 45.434954 132.915741 \nL 45.943074 132.514889 \nL 46.451193 132.333502 \nL 46.959313 132.879046 \nL 47.467432 132.772252 \nL 47.975552 132.853604 \nL 48.483672 132.761696 \nL 48.991791 135.080451 \nL 49.499911 135.110557 \nL 50.00803 134.790801 \nL 50.51615 136.042748 \nL 51.02427 135.454235 \nL 51.532389 137.520897 \nL 52.040509 137.311001 \nL 52.548628 138.871677 \nL 53.056748 137.231225 \nL 53.564868 139.519596 \nL 54.072987 139.539117 \nL 54.581107 139.390428 \nL 55.089226 139.363396 \nL 55.597346 141.26638 \nL 56.105466 141.680601 \nL 56.613585 141.79925 \nL 57.121705 142.07996 \nL 57.629824 141.024828 \nL 58.137944 140.775008 \nL 58.646064 142.838629 \nL 59.154183 142.108308 \nL 59.662303 142.559778 \nL 60.170422 143.993552 \nL 60.678542 144.626166 \nL 61.186661 144.455717 \nL 62.202901 145.032393 \nL 62.71102 145.904935 \nL 63.21914 146.062655 \nL 63.727259 145.130358 \nL 64.235379 145.959301 \nL 64.743499 145.295479 \nL 65.251618 146.115635 \nL 65.759738 146.584727 \nL 66.775977 146.605887 \nL 67.284097 146.142635 \nL 68.300336 147.968384 \nL 68.808455 148.585591 \nL 69.316575 147.100111 \nL 69.824695 149.549682 \nL 70.332814 148.931487 \nL 70.840934 148.514138 \nL 71.349053 148.789482 \nL 71.857173 149.611598 \nL 72.365293 150.133942 \nL 72.873412 149.107064 \nL 73.889651 150.758374 \nL 74.397771 147.589374 \nL 74.90589 151.462381 \nL 75.41401 150.543623 \nL 75.92213 151.651331 \nL 76.430249 150.694082 \nL 76.938369 151.1338 \nL 77.446488 150.813132 \nL 77.954608 151.416257 \nL 78.462728 151.504233 \nL 78.970847 152.075326 \nL 79.478967 151.377587 \nL 79.987086 152.216039 \nL 80.495206 152.807523 \nL 81.003326 151.395376 \nL 81.511445 153.140537 \nL 82.019565 153.354091 \nL 82.527684 155.383565 \nL 83.035804 153.652336 \nL 83.543924 154.414015 \nL 84.052043 153.871614 \nL 84.560163 154.495925 \nL 85.068282 154.631148 \nL 85.576402 155.060448 \nL 86.084522 153.305196 \nL 86.592641 155.427119 \nL 87.100761 155.846511 \nL 87.60888 155.101069 \nL 88.117 156.28445 \nL 88.62512 156.298734 \nL 89.133239 155.203244 \nL 89.641359 155.680605 \nL 90.149478 155.848998 \nL 90.657598 154.971017 \nL 91.165717 156.529577 \nL 91.673837 156.005231 \nL 92.181957 157.688588 \nL 92.690076 157.313435 \nL 93.198196 156.613005 \nL 93.706315 156.821454 \nL 94.214435 157.313287 \nL 94.722555 158.132421 \nL 95.230674 156.831048 \nL 95.738794 157.890904 \nL 96.755033 158.953036 \nL 97.263153 158.525405 \nL 97.771272 158.59367 \nL 98.279392 157.341555 \nL 98.787511 159.143727 \nL 99.295631 158.10437 \nL 99.803751 158.653564 \nL 100.31187 158.50855 \nL 101.328109 160.040848 \nL 101.836229 160.013368 \nL 102.344349 160.441753 \nL 102.852468 160.235527 \nL 103.360588 158.878903 \nL 103.868707 159.732085 \nL 104.376827 160.093503 \nL 104.884947 160.871423 \nL 105.393066 161.377869 \nL 105.901186 160.003526 \nL 106.409305 160.784505 \nL 106.917425 160.757641 \nL 107.425544 162.76814 \nL 108.441784 160.831337 \nL 108.949903 161.089443 \nL 109.458023 161.211453 \nL 109.966142 161.178253 \nL 110.474262 161.565434 \nL 110.982382 161.596615 \nL 111.490501 163.135063 \nL 111.998621 162.262498 \nL 112.50674 162.993679 \nL 113.01486 162.09505 \nL 113.52298 162.612325 \nL 114.539219 162.920447 \nL 115.047338 163.454668 \nL 116.063578 163.678173 \nL 116.571697 163.60871 \nL 117.079817 162.846323 \nL 117.587936 162.856938 \nL 118.096056 164.222112 \nL 118.604176 165.195621 \nL 119.112295 165.060936 \nL 119.620415 164.110158 \nL 120.128534 163.635554 \nL 120.636654 164.01459 \nL 121.652893 165.577125 \nL 122.669132 165.197653 \nL 123.177252 165.730104 \nL 123.685371 165.863378 \nL 124.193491 165.684119 \nL 124.701611 165.355487 \nL 125.20973 165.913608 \nL 125.71785 165.815058 \nL 126.225969 166.24522 \nL 126.734089 166.169747 \nL 127.750328 166.911956 \nL 128.258448 165.984852 \nL 128.766567 167.423026 \nL 129.274687 167.357791 \nL 130.290926 166.360567 \nL 130.799046 167.033009 \nL 131.307165 167.017423 \nL 131.815285 165.269838 \nL 132.323405 168.907154 \nL 132.831524 167.342653 \nL 133.339644 167.299097 \nL 133.847763 168.264749 \nL 134.355883 168.969954 \nL 134.864003 168.439112 \nL 135.372122 166.901326 \nL 135.880242 168.857482 \nL 136.388361 169.087552 \nL 136.896481 168.729238 \nL 137.4046 168.660195 \nL 137.91272 168.787875 \nL 138.42084 168.457251 \nL 138.928959 168.773358 \nL 139.437079 170.592123 \nL 139.945198 169.18537 \nL 140.453318 169.405421 \nL 140.961438 170.018107 \nL 141.469557 169.698636 \nL 141.977677 169.725221 \nL 142.485796 170.893403 \nL 142.993916 168.870897 \nL 143.502036 170.910773 \nL 144.010155 169.264496 \nL 144.518275 169.141082 \nL 145.026394 171.047217 \nL 145.534514 170.730648 \nL 146.042634 171.455855 \nL 146.550753 170.343016 \nL 147.566992 170.663498 \nL 148.075112 171.279368 \nL 148.583232 171.418753 \nL 149.091351 171.901955 \nL 149.599471 172.925259 \nL 150.10759 171.742801 \nL 150.61571 171.905819 \nL 151.631949 172.912831 \nL 152.140069 173.029565 \nL 152.648188 171.813537 \nL 153.156308 173.13943 \nL 153.664427 173.069802 \nL 154.172547 172.658665 \nL 154.680667 172.397073 \nL 155.188786 172.529122 \nL 156.205025 173.849005 \nL 156.713145 173.335309 \nL 157.729384 172.756079 \nL 158.237504 173.853696 \nL 158.745623 174.399972 \nL 159.253743 173.64133 \nL 159.761863 175.311949 \nL 160.269982 174.872079 \nL 161.286221 174.762946 \nL 161.794341 175.487757 \nL 162.302461 175.48258 \nL 162.81058 174.751319 \nL 163.3187 175.856675 \nL 163.826819 174.657573 \nL 164.843059 175.139642 \nL 165.351178 175.650408 \nL 165.859298 175.286907 \nL 166.367417 176.101246 \nL 166.875537 176.693859 \nL 167.891776 176.706616 \nL 168.399896 175.963872 \nL 168.908015 177.342138 \nL 169.416135 175.556574 \nL 169.924254 177.154181 \nL 170.432374 177.289715 \nL 170.940494 176.464706 \nL 171.448613 177.211588 \nL 171.956733 177.078559 \nL 172.464852 177.751285 \nL 172.972972 177.253122 \nL 173.481092 178.239931 \nL 173.989211 176.91339 \nL 174.497331 178.4709 \nL 175.00545 178.767382 \nL 175.51357 178.652031 \nL 176.02169 177.182935 \nL 177.037929 178.165349 \nL 177.546048 178.028551 \nL 178.054168 179.339696 \nL 178.562288 178.489402 \nL 179.070407 178.769488 \nL 179.578527 177.760788 \nL 180.086646 179.289796 \nL 181.102885 178.420817 \nL 181.611005 179.661155 \nL 182.119125 180.20273 \nL 182.627244 179.392095 \nL 183.135364 180.174541 \nL 183.643483 180.064498 \nL 184.151603 180.427593 \nL 184.659723 180.386996 \nL 185.167842 180.82694 \nL 185.675962 179.229421 \nL 186.184081 179.902747 \nL 186.692201 181.281289 \nL 187.200321 180.819496 \nL 187.70844 181.272114 \nL 188.21656 180.973158 \nL 188.724679 181.998729 \nL 189.232799 181.515205 \nL 189.740919 181.349621 \nL 190.249038 181.006494 \nL 191.773397 182.224811 \nL 192.281517 181.549761 \nL 192.789636 181.540815 \nL 193.805875 182.444791 \nL 194.822115 182.456202 \nL 195.330234 182.038664 \nL 195.838354 183.252102 \nL 196.346473 182.579181 \nL 196.854593 182.827707 \nL 197.362712 181.581185 \nL 197.870832 182.655813 \nL 198.378952 182.2788 \nL 198.887071 184.212506 \nL 199.395191 183.370265 \nL 199.90331 183.529833 \nL 200.41143 183.934162 \nL 200.91955 184.118832 \nL 201.427669 184.803413 \nL 201.935789 184.328394 \nL 202.443908 184.602283 \nL 202.952028 185.433943 \nL 203.460148 184.39046 \nL 203.968267 184.661174 \nL 204.476387 184.566715 \nL 204.984506 184.020024 \nL 205.492626 184.947361 \nL 206.000746 184.693526 \nL 206.508865 185.081385 \nL 207.016985 185.095356 \nL 207.525104 185.375112 \nL 208.033224 185.477546 \nL 208.541344 184.478348 \nL 209.049463 185.583062 \nL 209.557583 187.273473 \nL 210.065702 186.956184 \nL 210.573822 186.10652 \nL 211.081941 185.745408 \nL 211.590061 187.17635 \nL 212.098181 186.004601 \nL 212.6063 185.44085 \nL 213.622539 187.310951 \nL 214.130659 186.77146 \nL 214.638779 187.014465 \nL 215.146898 186.957555 \nL 215.655018 188.034394 \nL 216.163137 188.447075 \nL 217.179377 187.148525 \nL 217.687496 187.496603 \nL 218.195616 187.678351 \nL 218.703735 187.651081 \nL 219.211855 189.305726 \nL 219.719975 188.073219 \nL 220.228094 188.278379 \nL 220.736214 188.922637 \nL 221.244333 189.067072 \nL 221.752453 188.481824 \nL 222.260573 188.372914 \nL 223.276812 189.334796 \nL 223.784931 189.930238 \nL 224.293051 189.356851 \nL 224.801171 190.297874 \nL 225.30929 189.828265 \nL 225.81741 190.071385 \nL 226.325529 189.988135 \nL 226.833649 189.632443 \nL 227.341768 189.968424 \nL 227.849888 190.477811 \nL 228.358008 190.140161 \nL 228.866127 190.852285 \nL 229.374247 190.072595 \nL 229.882366 190.785356 \nL 230.390486 191.214335 \nL 230.898606 190.839905 \nL 231.406725 191.246999 \nL 231.914845 191.446332 \nL 232.422964 191.511944 \nL 232.931084 190.277504 \nL 233.439204 190.695721 \nL 233.947323 190.200368 \nL 234.455443 192.100392 \nL 234.963562 190.883904 \nL 235.471682 192.230505 \nL 235.979802 192.288212 \nL 236.487921 193.100498 \nL 236.996041 192.262233 \nL 237.50416 190.798738 \nL 238.01228 193.295177 \nL 238.5204 193.567659 \nL 239.028519 192.612253 \nL 239.536639 192.258753 \nL 240.044758 191.739566 \nL 240.552878 193.275995 \nL 241.060997 192.716244 \nL 241.569117 192.876572 \nL 242.077237 192.459233 \nL 242.585356 193.390304 \nL 243.093476 193.75448 \nL 243.601595 194.259746 \nL 244.109715 193.443958 \nL 245.125954 195.146225 \nL 246.142193 193.002542 \nL 246.650313 193.659954 \nL 247.158433 193.518306 \nL 247.666552 194.630314 \nL 248.682791 194.869041 \nL 249.190911 194.029071 \nL 249.699031 195.390811 \nL 250.20715 195.685459 \nL 250.71527 195.275712 \nL 251.731509 195.088091 \nL 252.747748 197.006564 \nL 253.255868 196.932662 \nL 253.763987 196.286919 \nL 254.272107 196.426437 \nL 254.780227 195.901767 \nL 255.288346 195.906903 \nL 255.796466 195.348539 \nL 256.304585 195.773225 \nL 256.812705 196.675313 \nL 257.320824 197.322979 \nL 257.828944 196.117196 \nL 258.337064 198.092202 \nL 258.845183 196.631939 \nL 259.353303 197.08431 \nL 259.861422 196.569363 \nL 260.369542 196.657697 \nL 260.877662 198.403338 \nL 261.893901 197.807563 \nL 262.40202 198.493379 \nL 262.91014 198.139199 \nL 263.41826 198.608079 \nL 263.926379 199.238691 \nL 264.434499 197.741455 \nL 264.942618 198.379732 \nL 265.450738 197.737534 \nL 265.958858 199.191409 \nL 266.466977 199.581175 \nL 266.975097 198.264795 \nL 267.483216 198.723304 \nL 267.991336 199.874583 \nL 268.499456 198.669711 \nL 269.007575 198.531679 \nL 269.515695 198.757257 \nL 270.023814 198.629729 \nL 270.531934 198.909415 \nL 271.040053 199.604281 \nL 271.548173 200.769804 \nL 272.056293 200.580175 \nL 272.564412 199.129174 \nL 273.072532 199.402026 \nL 273.580651 200.767535 \nL 274.088771 200.361699 \nL 274.596891 200.835406 \nL 275.10501 200.442282 \nL 275.61313 200.481405 \nL 276.121249 201.145356 \nL 276.629369 200.968158 \nL 277.137489 201.217531 \nL 277.645608 200.874961 \nL 278.153728 201.166207 \nL 278.661847 202.054483 \nL 279.169967 202.301886 \nL 280.186206 201.64587 \nL 280.694326 201.145482 \nL 281.202445 201.327431 \nL 281.710565 201.972763 \nL 282.218685 201.956281 \nL 282.726804 202.854259 \nL 283.234924 201.573563 \nL 284.251163 202.859822 \nL 284.759283 203.076524 \nL 285.267402 203.582407 \nL 285.775522 203.259539 \nL 286.283641 202.439549 \nL 286.791761 202.938847 \nL 287.29988 202.890868 \nL 287.808 204.151457 \nL 288.31612 203.419221 \nL 288.824239 203.286488 \nL 289.332359 202.856495 \nL 290.856718 204.720722 \nL 291.364837 203.975113 \nL 292.381076 204.705968 \nL 292.889196 204.722179 \nL 293.397316 204.213088 \nL 293.905435 203.9922 \nL 294.413555 204.271263 \nL 295.429794 205.597631 \nL 295.937914 204.446419 \nL 296.446033 205.786529 \nL 296.954153 204.752539 \nL 297.462272 205.105052 \nL 298.478512 206.581878 \nL 298.986631 205.295299 \nL 299.494751 205.353628 \nL 300.00287 204.934029 \nL 300.51099 205.893084 \nL 301.01911 205.736279 \nL 301.527229 206.989365 \nL 302.035349 206.008025 \nL 302.543468 206.753124 \nL 303.051588 206.287082 \nL 303.559707 206.114166 \nL 304.067827 207.344207 \nL 304.575947 206.42866 \nL 305.084066 208.030339 \nL 306.100305 206.950908 \nL 306.608425 206.866951 \nL 307.116545 207.2069 \nL 307.624664 205.863215 \nL 308.132784 208.461441 \nL 308.640903 207.444407 \nL 309.149023 207.222907 \nL 309.657143 208.09207 \nL 310.165262 207.149579 \nL 310.673382 207.434456 \nL 311.181501 208.642657 \nL 311.689621 208.942997 \nL 312.197741 208.435452 \nL 312.70586 208.973219 \nL 313.21398 208.34881 \nL 313.722099 208.504074 \nL 314.230219 208.973594 \nL 314.738339 208.508326 \nL 315.246458 209.317704 \nL 315.754578 209.040225 \nL 316.262697 210.268521 \nL 317.278936 208.694623 \nL 317.787056 210.038333 \nL 318.295176 208.983716 \nL 318.803295 209.85994 \nL 319.311415 209.561648 \nL 320.327654 210.101903 \nL 320.835774 209.970726 \nL 321.343893 210.607629 \nL 321.852013 210.072965 \nL 322.360132 210.913692 \nL 322.868252 210.446059 \nL 323.884491 211.579477 \nL 324.90073 210.00187 \nL 325.40885 210.810224 \nL 325.91697 211.151288 \nL 326.425089 212.373993 \nL 326.933209 212.354501 \nL 327.441328 210.756289 \nL 327.949448 210.815903 \nL 328.457568 211.855153 \nL 328.965687 212.167843 \nL 329.473807 211.380562 \nL 329.981926 211.538336 \nL 330.490046 212.267252 \nL 330.998166 211.506109 \nL 331.506285 212.717836 \nL 332.014405 212.5737 \nL 332.522524 212.03426 \nL 333.030644 211.848029 \nL 333.538763 212.626686 \nL 334.046883 212.751269 \nL 334.555003 213.367403 \nL 335.063122 213.143119 \nL 335.571242 212.765053 \nL 336.079361 212.661345 \nL 336.587481 213.052069 \nL 337.095601 212.987411 \nL 337.60372 213.465604 \nL 338.11184 213.553358 \nL 338.619959 212.981711 \nL 339.128079 213.999723 \nL 339.636199 214.756364 \nL 340.144318 213.354506 \nL 340.144318 213.354506 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 20.5625 224.64 \nL 20.5625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 355.3625 224.64 \nL 355.3625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 20.5625 224.64 \nL 355.3625 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 20.5625 7.2 \nL 355.3625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pe65f65a04e\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"20.5625\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(lossvalues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting nltk\n",
      "  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 1.1MB/s \n",
      "\u001b[?25hCollecting click (from nltk)\n",
      "  Downloading https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl (82kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 7.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk)\n",
      "Collecting regex (from nltk)\n",
      "  Downloading https://files.pythonhosted.org/packages/54/3a/4ce9e3b330e174eebb2fbfa6d66d89fa26db3db4f112fa4009f431e2662f/regex-2021.3.17-cp36-cp36m-manylinux1_x86_64.whl (666kB)\n",
      "\u001b[K    100% |████████████████████████████████| 675kB 2.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk)\n",
      "Building wheels for collected packages: nltk\n",
      "  Running setup.py bdist_wheel for nltk ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
      "Successfully built nltk\n",
      "Installing collected packages: click, regex, nltk\n",
      "Successfully installed click-7.1.2 nltk-3.5 regex-2021.3.17\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 0.141080287481769 0.04887164517296948\n",
      "1000 0.1516007730399172 0.06212795053598718\n",
      "2000 0.1484027583953258 0.06274910396045973\n",
      "3000 0.14842752311376667 0.06245033980700839\n",
      "4000 0.14967529771287516 0.06210185892314326\n",
      "5000 0.14995930376568595 0.06275583953236272\n",
      "6000 0.14816663596209623 0.06125312676338697\n",
      "7000 0.1522370528810268 0.06461756309961207\n",
      "8000 0.15195286809183559 0.06484537039782245\n",
      "Total Bleu Score for 1 grams on testing pairs:  0.15151436437488666\n",
      "Total Bleu Score for 2 grams on testing pairs:  0.06460218410379305\n"
     ]
    }
   ],
   "source": [
    "#  Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "from nltk.translate.bleu_score import sentence_bleu,corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "gram1_bleu_score = []\n",
    "gram2_bleu_score = []\n",
    "for i in range(0,len(testpairs),1):\n",
    "  \n",
    "  input_sentence = testpairs[i][0]\n",
    "  \n",
    "  reference = testpairs[i][1:]\n",
    "  templist = []\n",
    "  for k in range(len(reference)):\n",
    "    if(reference[k]!=''):\n",
    "      temp = reference[k].split(' ')\n",
    "      templist.append(temp)\n",
    "  \n",
    "  \n",
    "  input_sentence = normalizeString(input_sentence)\n",
    "  output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "  output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "  chencherry = SmoothingFunction()\n",
    "#   print(output_words)\n",
    "#   print(templist)\n",
    "  score1 = sentence_bleu(templist,output_words,weights=(1, 0, 0, 0) ,smoothing_function=chencherry.method1)\n",
    "  score2 = sentence_bleu(templist,output_words,weights=(0.5, 0.5, 0, 0),smoothing_function=chencherry.method1) \n",
    "  gram1_bleu_score.append(score1)\n",
    "  gram2_bleu_score.append(score2)\n",
    "  if i%1000 == 0:\n",
    "    print(i,sum(gram1_bleu_score)/len(gram1_bleu_score),sum(gram2_bleu_score)/len(gram2_bleu_score))\n",
    "print(\"Total Bleu Score for 1 grams on testing pairs: \", sum(gram1_bleu_score)/len(gram1_bleu_score) )  \n",
    "print(\"Total Bleu Score for 2 grams on testing pairs: \", sum(gram2_bleu_score)/len(gram2_bleu_score) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bot: it s me . . .\n",
      "Bot: it s me . . .\n",
      "Bot: i know . s the radio !\n",
      "Bot: i don t know . . .\n",
      "Bot: feeding ! . . .\n",
      "Bot: what ? ? ? ?\n",
      "Bot: you re not a awful risk ! !\n",
      "Error: Encountered unknown word.\n",
      "Error: Encountered unknown word.\n",
      "Bot: now . . . . . .\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: hello . ? ! ?\n",
      "Bot: what ? ? ! ?\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-ed6594a68986>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Begin chatting (uncomment and run the following line to begin)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mevaluateInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Hi, how are you?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-ec18c2cf8348>\u001b[0m in \u001b[0;36mevaluateInput\u001b[0;34m(encoder, decoder, searcher, voc)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0minput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'> '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'q'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m         )\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluateInput(encoder, decoder, searcher, voc)\n",
    "# input\n",
    "# Hi, how are you?\n",
    "# What\n",
    "# I don't understand you\n",
    "# hmm, good bye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    def __init__(self, decoder_hidden, last_idx=SOS_token, sentence_idxes=[], sentence_scores=[]):\n",
    "        if(len(sentence_idxes) != len(sentence_scores)):\n",
    "            raise ValueError(\"length of indexes and scores should be the same\")\n",
    "        self.decoder_hidden = decoder_hidden\n",
    "        self.last_idx = last_idx\n",
    "        self.sentence_idxes =  sentence_idxes\n",
    "        self.sentence_scores = sentence_scores\n",
    "\n",
    "    def avgScore(self):\n",
    "        if len(self.sentence_scores) == 0:\n",
    "            raise ValueError(\"Calculate average score of sentence, but got no word\")\n",
    "        # return mean of sentence_score\n",
    "        return sum(self.sentence_scores) / len(self.sentence_scores)\n",
    "\n",
    "    def addTopk(self, topi, topv, decoder_hidden, beam_size, voc):\n",
    "        topv = torch.log(topv)\n",
    "        terminates, sentences = [], []\n",
    "        for i in range(beam_size):\n",
    "            if topi[0][i] == EOS_token:\n",
    "                terminates.append(([voc.index2word[idx.item()] for idx in self.sentence_idxes] + ['<EOS>'],\n",
    "                                   self.avgScore())) \n",
    "                continue\n",
    "            idxes = self.sentence_idxes[:] \n",
    "            scores = self.sentence_scores[:] \n",
    "            idxes.append(topi[0][i])\n",
    "            scores.append(topv[0][i])\n",
    "            sentences.append(Sentence(decoder_hidden, topi[0][i], idxes, scores))\n",
    "        return terminates, sentences\n",
    "\n",
    "    def toWordScore(self, voc):\n",
    "        \n",
    "        words = []\n",
    "        for i in range(len(self.sentence_idxes)):\n",
    "            if self.sentence_idxes[i] == EOS_token:\n",
    "                words.append('<EOS>')\n",
    "            else:\n",
    "                words.append(voc.index2word[self.sentence_idxes[i].item()])\n",
    "       \n",
    "        if self.sentence_idxes[-1] != EOS_token:\n",
    "            words.append('<EOS>')\n",
    "        return (words, self.avgScore())\n",
    "\n",
    "    def __repr__(self):\n",
    "        res = f\"Sentence with indices {self.sentence_idxes} \"\n",
    "        res += f\"and scores {self.sentence_scores}\"\n",
    "        return res\n",
    "def beam_decode(decoder, decoder_hidden, encoder_outputs, voc, beam_size, max_length=MAX_LENGTH):\n",
    "    terminal_sentences, prev_top_sentences, next_top_sentences = [], [], []\n",
    "    prev_top_sentences.append(Sentence(decoder_hidden))\n",
    "    for i in range(max_length):\n",
    "        \n",
    "        for sentence in prev_top_sentences:\n",
    "            decoder_input = torch.LongTensor([[sentence.last_idx]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "\n",
    "            decoder_hidden = sentence.decoder_hidden\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(beam_size)\n",
    "            term, top = sentence.addTopk(topi, topv, decoder_hidden, beam_size, voc)\n",
    "            terminal_sentences.extend(term)\n",
    "            next_top_sentences.extend(top)\n",
    "           \n",
    "        \n",
    "        next_top_sentences.sort(key=lambda s: s.avgScore(), reverse=True)\n",
    "        prev_top_sentences = next_top_sentences[:beam_size]\n",
    "        next_top_sentences = []\n",
    "        \n",
    "\n",
    "    terminal_sentences += [sentence.toWordScore(voc) for sentence in prev_top_sentences]\n",
    "    terminal_sentences.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    n = min(len(terminal_sentences), 15)\n",
    "    return terminal_sentences[:n]\n",
    "\n",
    "\n",
    "\n",
    "class BeamSearchDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, voc, beam_size=10):\n",
    "        super(BeamSearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.voc = voc\n",
    "        self.beam_size = beam_size\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden[:self.decoder.n_layers]\n",
    "        \n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        sentences = beam_decode(self.decoder, decoder_hidden, encoder_outputs, self.voc, self.beam_size, max_length)\n",
    "        \n",
    "        \n",
    "        all_tokens = [torch.tensor(self.voc.word2index.get(w, 0)) for w in sentences[0][0]]\n",
    "        return all_tokens, None\n",
    "\n",
    "    def __str__(self):\n",
    "        res = f\"BeamSearchDecoder with beam size {self.beam_size}\"\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "\n",
    "hidden_size = 512\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.5\n",
    "batch_size = 256 \n",
    "loadFilename = None\n",
    "\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ons.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:38: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "Iteration: 6000; Percent complete: 100.0%; Average loss: 1.8197\n",
      "content/cb_model/Chat/2-4_512\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'content/'\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 6000\n",
    "print_every = 10\n",
    "save_every = 2000\n",
    "loadFilename = None\n",
    "corpus_name=\"Chat\"\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "print(\"Starting Training!\")\n",
    "lossvalues = trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 362.5625 248.518125\" width=\"362.5625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-02T10:09:21.445741</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 362.5625 248.518125 \nL 362.5625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 20.5625 224.64 \nL 355.3625 224.64 \nL 355.3625 7.2 \nL 20.5625 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m5a2c315d6b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.780682\" xlink:href=\"#m5a2c315d6b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(32.599432 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"86.592641\" xlink:href=\"#m5a2c315d6b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(77.048891 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"137.4046\" xlink:href=\"#m5a2c315d6b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(127.86085 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"188.21656\" xlink:href=\"#m5a2c315d6b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(178.67281 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"239.028519\" xlink:href=\"#m5a2c315d6b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(229.484769 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"289.840478\" xlink:href=\"#m5a2c315d6b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(280.296728 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"340.652438\" xlink:href=\"#m5a2c315d6b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 600 -->\n      <g transform=\"translate(331.108688 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m1b17a6a227\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m1b17a6a227\" y=\"208.512406\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 2 -->\n      <g transform=\"translate(7.2 212.311624)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m1b17a6a227\" y=\"177.94323\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 3 -->\n      <g transform=\"translate(7.2 181.742449)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m1b17a6a227\" y=\"147.374055\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 4 -->\n      <g transform=\"translate(7.2 151.173274)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m1b17a6a227\" y=\"116.804879\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 5 -->\n      <g transform=\"translate(7.2 120.604098)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m1b17a6a227\" y=\"86.235704\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 6 -->\n      <g transform=\"translate(7.2 90.034923)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m1b17a6a227\" y=\"55.666529\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 7 -->\n      <g transform=\"translate(7.2 59.465747)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m1b17a6a227\" y=\"25.097353\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 8 -->\n      <g transform=\"translate(7.2 28.896572)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#pf800ec4f45)\" d=\"M 35.780682 17.083636 \nL 36.288801 91.896812 \nL 36.796921 115.894549 \nL 37.305041 115.310874 \nL 37.81316 119.811547 \nL 38.32128 120.647251 \nL 38.829399 121.937219 \nL 39.845639 123.266266 \nL 40.353758 122.594327 \nL 40.861878 125.087294 \nL 41.369997 125.340453 \nL 41.878117 125.344867 \nL 42.894356 124.812083 \nL 43.910595 127.432403 \nL 44.926834 127.23274 \nL 45.434954 127.660765 \nL 45.943074 127.664187 \nL 46.451193 126.536657 \nL 46.959313 128.289461 \nL 47.467432 128.441256 \nL 47.975552 128.187828 \nL 48.483672 128.204354 \nL 48.991791 128.385698 \nL 50.00803 134.151289 \nL 50.51615 134.626282 \nL 51.02427 135.359591 \nL 51.532389 135.320805 \nL 52.040509 137.042001 \nL 53.564868 135.914004 \nL 54.072987 136.839748 \nL 54.581107 137.172213 \nL 55.597346 138.519978 \nL 56.105466 138.916917 \nL 56.613585 137.860767 \nL 57.121705 138.998925 \nL 58.137944 140.141388 \nL 58.646064 140.092359 \nL 59.154183 141.042502 \nL 59.662303 140.599137 \nL 60.170422 142.13824 \nL 61.186661 141.879572 \nL 61.694781 142.098774 \nL 62.202901 143.996163 \nL 62.71102 143.516743 \nL 63.21914 143.848098 \nL 63.727259 143.324905 \nL 64.235379 144.001763 \nL 64.743499 146.135375 \nL 65.251618 145.473418 \nL 65.759738 145.748204 \nL 66.267857 144.651532 \nL 66.775977 145.641881 \nL 67.284097 145.844395 \nL 67.792216 146.622419 \nL 68.300336 144.830935 \nL 68.808455 147.402071 \nL 69.316575 147.634886 \nL 69.824695 146.371614 \nL 70.332814 146.972093 \nL 70.840934 147.377021 \nL 71.349053 148.093517 \nL 71.857173 147.747058 \nL 72.365293 147.234927 \nL 73.381532 148.732187 \nL 73.889651 148.274067 \nL 74.397771 149.08166 \nL 74.90589 149.307677 \nL 75.41401 148.992769 \nL 75.92213 148.976686 \nL 76.430249 150.92457 \nL 76.938369 150.135203 \nL 77.446488 149.828772 \nL 77.954608 150.496407 \nL 78.462728 148.910222 \nL 78.970847 150.403607 \nL 79.478967 150.532183 \nL 79.987086 150.792451 \nL 80.495206 151.303548 \nL 81.003326 150.883514 \nL 81.511445 151.057472 \nL 82.019565 151.891986 \nL 83.035804 151.283265 \nL 83.543924 151.285283 \nL 84.052043 152.049174 \nL 84.560163 152.393902 \nL 85.068282 152.0477 \nL 85.576402 152.864248 \nL 86.592641 153.938663 \nL 87.100761 153.271449 \nL 87.60888 152.841794 \nL 88.117 152.190809 \nL 88.62512 154.821703 \nL 89.133239 153.153177 \nL 89.641359 153.207479 \nL 90.149478 153.407271 \nL 90.657598 154.894989 \nL 91.165717 153.620035 \nL 91.673837 154.299578 \nL 92.181957 155.493033 \nL 92.690076 154.099216 \nL 93.198196 154.91429 \nL 94.214435 155.60894 \nL 94.722555 155.36368 \nL 95.230674 154.984195 \nL 95.738794 155.5428 \nL 96.246913 155.665716 \nL 96.755033 156.938419 \nL 97.263153 156.241026 \nL 97.771272 157.581357 \nL 98.279392 156.250927 \nL 98.787511 156.986952 \nL 99.295631 156.227602 \nL 99.803751 157.344474 \nL 100.31187 155.709983 \nL 100.81999 157.612937 \nL 101.328109 157.889773 \nL 101.836229 157.739033 \nL 102.344349 158.264035 \nL 102.852468 159.05751 \nL 103.360588 159.238904 \nL 103.868707 157.982668 \nL 104.376827 157.894544 \nL 104.884947 158.048184 \nL 105.393066 157.765561 \nL 105.901186 159.002358 \nL 106.409305 161.082968 \nL 106.917425 158.598138 \nL 107.425544 159.269835 \nL 107.933664 159.160587 \nL 108.441784 160.217124 \nL 108.949903 160.645147 \nL 109.458023 160.479961 \nL 109.966142 159.103198 \nL 110.474262 161.735289 \nL 110.982382 161.935365 \nL 111.490501 161.194604 \nL 111.998621 161.396182 \nL 112.50674 160.507692 \nL 113.01486 161.496017 \nL 114.031099 162.411174 \nL 114.539219 162.403993 \nL 115.047338 162.658286 \nL 115.555458 163.711389 \nL 116.063578 161.381218 \nL 116.571697 163.205603 \nL 117.079817 163.702742 \nL 117.587936 162.073089 \nL 118.604176 164.005804 \nL 119.112295 163.770955 \nL 119.620415 162.363822 \nL 120.128534 164.026766 \nL 120.636654 163.585928 \nL 121.144773 163.96563 \nL 121.652893 163.113793 \nL 122.161013 164.46495 \nL 122.669132 164.608179 \nL 123.177252 164.095269 \nL 124.193491 164.857254 \nL 124.701611 165.436372 \nL 125.20973 164.685967 \nL 125.71785 164.241189 \nL 126.225969 164.947691 \nL 126.734089 166.504896 \nL 127.242209 165.433807 \nL 127.750328 164.976545 \nL 128.258448 165.654098 \nL 128.766567 166.622025 \nL 129.274687 166.149002 \nL 130.290926 166.918351 \nL 130.799046 167.657117 \nL 131.307165 166.785294 \nL 131.815285 167.772681 \nL 132.323405 167.539256 \nL 132.831524 166.698146 \nL 133.339644 167.053894 \nL 134.355883 166.671962 \nL 134.864003 167.481373 \nL 135.372122 167.750418 \nL 135.880242 168.160826 \nL 136.388361 168.34147 \nL 136.896481 168.376247 \nL 137.4046 169.306666 \nL 137.91272 168.576276 \nL 138.42084 168.066216 \nL 138.928959 168.498326 \nL 139.437079 167.953919 \nL 139.945198 168.810621 \nL 140.453318 169.067588 \nL 140.961438 169.624262 \nL 141.469557 169.162732 \nL 141.977677 170.030857 \nL 142.485796 169.224799 \nL 142.993916 169.720467 \nL 143.502036 170.400936 \nL 144.010155 169.597334 \nL 144.518275 170.569428 \nL 145.534514 169.697817 \nL 146.042634 170.730246 \nL 146.550753 169.990718 \nL 147.566992 171.683896 \nL 148.075112 171.059701 \nL 148.583232 171.455864 \nL 149.091351 172.193278 \nL 149.599471 170.47201 \nL 150.10759 173.044521 \nL 150.61571 171.755302 \nL 152.140069 172.383531 \nL 152.648188 172.439353 \nL 153.664427 171.286214 \nL 154.172547 172.477525 \nL 154.680667 172.94956 \nL 155.188786 173.782023 \nL 155.696906 171.986158 \nL 156.205025 174.532725 \nL 156.713145 173.228518 \nL 157.221265 172.936168 \nL 157.729384 173.987823 \nL 158.237504 173.811647 \nL 158.745623 173.138933 \nL 159.253743 174.860603 \nL 159.761863 174.072747 \nL 160.269982 174.349803 \nL 160.778102 172.910332 \nL 161.286221 174.545302 \nL 161.794341 174.676074 \nL 162.302461 175.10492 \nL 162.81058 174.639102 \nL 163.3187 175.328886 \nL 163.826819 174.260366 \nL 164.334939 175.215393 \nL 164.843059 176.450238 \nL 165.351178 176.188923 \nL 165.859298 176.764273 \nL 166.367417 176.617008 \nL 166.875537 176.183505 \nL 167.383656 174.815615 \nL 167.891776 175.660926 \nL 168.399896 175.303469 \nL 168.908015 176.747715 \nL 169.416135 176.276291 \nL 169.924254 176.400347 \nL 170.432374 177.447849 \nL 170.940494 176.852788 \nL 171.448613 177.064066 \nL 171.956733 176.612276 \nL 172.464852 176.551787 \nL 172.972972 177.218589 \nL 173.481092 178.36634 \nL 173.989211 177.118829 \nL 174.497331 177.417034 \nL 175.00545 177.568905 \nL 175.51357 178.368166 \nL 176.02169 178.478871 \nL 176.529809 177.526734 \nL 177.546048 178.818645 \nL 178.054168 178.682524 \nL 178.562288 178.425989 \nL 179.070407 178.394899 \nL 179.578527 179.899152 \nL 180.086646 179.763687 \nL 180.594766 178.579984 \nL 181.102885 180.112498 \nL 181.611005 179.709895 \nL 182.119125 179.682868 \nL 182.627244 179.995752 \nL 183.135364 180.580531 \nL 183.643483 179.030712 \nL 184.151603 180.408272 \nL 184.659723 180.056881 \nL 185.167842 180.287339 \nL 186.184081 180.975108 \nL 186.692201 181.002515 \nL 187.200321 181.702301 \nL 187.70844 181.019055 \nL 188.21656 181.591134 \nL 188.724679 180.555693 \nL 189.232799 181.574911 \nL 189.740919 181.686585 \nL 190.249038 181.597139 \nL 190.757158 180.940967 \nL 191.265277 180.517837 \nL 191.773397 181.137705 \nL 192.281517 183.111998 \nL 192.789636 182.321646 \nL 193.297756 181.928802 \nL 193.805875 182.566801 \nL 194.313995 181.962689 \nL 194.822115 183.38935 \nL 195.330234 182.658219 \nL 195.838354 182.126258 \nL 196.346473 183.340661 \nL 197.362712 183.445538 \nL 197.870832 182.954878 \nL 198.378952 182.990665 \nL 198.887071 182.876024 \nL 199.395191 183.303331 \nL 199.90331 184.653927 \nL 200.41143 183.725205 \nL 201.427669 185.579512 \nL 201.935789 184.465975 \nL 202.443908 185.635807 \nL 202.952028 184.936528 \nL 203.460148 185.839401 \nL 203.968267 184.146451 \nL 204.476387 184.104876 \nL 204.984506 185.107689 \nL 205.492626 185.427747 \nL 206.000746 186.072358 \nL 207.525104 185.768627 \nL 208.033224 186.03923 \nL 208.541344 185.69489 \nL 209.049463 186.384715 \nL 209.557583 185.721647 \nL 210.573822 187.696438 \nL 211.081941 186.957109 \nL 211.590061 186.665116 \nL 212.098181 187.728613 \nL 213.11442 187.760966 \nL 213.622539 186.86718 \nL 214.638779 187.963001 \nL 215.146898 187.168181 \nL 215.655018 188.637535 \nL 216.671257 187.134691 \nL 217.179377 188.387863 \nL 217.687496 187.759137 \nL 218.195616 188.424891 \nL 218.703735 189.320378 \nL 219.719975 188.560343 \nL 220.228094 189.214913 \nL 220.736214 189.115417 \nL 221.244333 188.592326 \nL 221.752453 189.733354 \nL 222.260573 189.196881 \nL 222.768692 189.388167 \nL 223.276812 190.24047 \nL 223.784931 190.218547 \nL 224.293051 189.162432 \nL 224.801171 189.771823 \nL 225.30929 189.020548 \nL 225.81741 190.491356 \nL 226.325529 190.310236 \nL 226.833649 190.800677 \nL 227.341768 190.621641 \nL 227.849888 191.05513 \nL 228.358008 190.498995 \nL 228.866127 190.577074 \nL 229.374247 191.475641 \nL 229.882366 191.991875 \nL 230.390486 191.197729 \nL 230.898606 191.617524 \nL 231.406725 191.100693 \nL 231.914845 192.116127 \nL 232.422964 191.78509 \nL 232.931084 192.633053 \nL 233.439204 191.831398 \nL 233.947323 191.685086 \nL 235.471682 192.569978 \nL 235.979802 193.445553 \nL 236.487921 192.465962 \nL 236.996041 192.894758 \nL 237.50416 193.011655 \nL 238.01228 194.397068 \nL 238.5204 194.119361 \nL 239.028519 193.40974 \nL 239.536639 193.137265 \nL 240.044758 193.593632 \nL 240.552878 192.612044 \nL 241.060997 194.296246 \nL 241.569117 192.940785 \nL 242.077237 194.025025 \nL 242.585356 194.280233 \nL 243.093476 193.985909 \nL 243.601595 195.173215 \nL 244.109715 195.091619 \nL 244.617835 193.982562 \nL 245.125954 193.807921 \nL 245.634074 194.831311 \nL 246.142193 194.840222 \nL 246.650313 194.570968 \nL 247.158433 195.555956 \nL 247.666552 196.150858 \nL 248.174672 195.453336 \nL 248.682791 196.534406 \nL 249.190911 196.082154 \nL 249.699031 195.782557 \nL 250.20715 195.726544 \nL 250.71527 196.632553 \nL 251.223389 195.02633 \nL 251.731509 195.216261 \nL 252.747748 196.797996 \nL 253.255868 196.526351 \nL 253.763987 197.057231 \nL 254.272107 195.864443 \nL 254.780227 197.133903 \nL 255.288346 196.934102 \nL 255.796466 197.685087 \nL 256.304585 197.632642 \nL 256.812705 197.195685 \nL 257.320824 197.378378 \nL 257.828944 196.926788 \nL 258.337064 197.468035 \nL 258.845183 197.429433 \nL 259.353303 197.793859 \nL 259.861422 197.991197 \nL 260.369542 197.718464 \nL 260.877662 197.937619 \nL 261.385781 197.953507 \nL 262.40202 199.414511 \nL 262.91014 198.922567 \nL 263.41826 199.214614 \nL 263.926379 197.976628 \nL 264.434499 198.360627 \nL 264.942618 199.387295 \nL 265.450738 198.6683 \nL 265.958858 199.2389 \nL 266.466977 199.360086 \nL 266.975097 198.600863 \nL 267.483216 199.454876 \nL 267.991336 197.919537 \nL 268.499456 199.690205 \nL 269.007575 200.099818 \nL 269.515695 200.268213 \nL 270.023814 200.991808 \nL 271.040053 200.03331 \nL 271.548173 201.110747 \nL 272.056293 201.031839 \nL 272.564412 201.468275 \nL 273.072532 201.229141 \nL 273.580651 200.813245 \nL 274.088771 201.855818 \nL 274.596891 200.855278 \nL 275.10501 201.361505 \nL 275.61313 202.623947 \nL 276.121249 202.229472 \nL 276.629369 201.609649 \nL 277.137489 201.708448 \nL 277.645608 203.177849 \nL 278.153728 202.799862 \nL 278.661847 202.162671 \nL 279.169967 202.398813 \nL 279.678087 203.335124 \nL 280.186206 202.666234 \nL 280.694326 203.165341 \nL 281.202445 202.267235 \nL 281.710565 202.621489 \nL 282.218685 203.123133 \nL 282.726804 203.955862 \nL 283.234924 203.028487 \nL 284.251163 203.197269 \nL 284.759283 203.674914 \nL 285.267402 203.378508 \nL 285.775522 203.559806 \nL 286.283641 204.174012 \nL 286.791761 203.318093 \nL 287.29988 203.942827 \nL 287.808 205.101972 \nL 288.31612 204.117229 \nL 288.824239 203.983301 \nL 289.332359 204.813087 \nL 289.840478 203.727703 \nL 290.348598 204.91819 \nL 290.856718 205.296725 \nL 291.364837 204.920509 \nL 291.872957 205.056041 \nL 292.381076 205.703468 \nL 292.889196 206.582921 \nL 293.397316 205.326438 \nL 293.905435 205.818968 \nL 294.413555 205.1173 \nL 294.921674 205.17721 \nL 295.429794 205.683751 \nL 295.937914 205.428195 \nL 296.446033 205.401511 \nL 296.954153 205.872221 \nL 297.462272 205.991632 \nL 297.970392 205.458903 \nL 298.986631 206.348992 \nL 299.494751 206.306292 \nL 300.00287 207.223593 \nL 300.51099 207.688814 \nL 301.01911 206.567897 \nL 301.527229 207.461092 \nL 302.035349 207.123598 \nL 302.543468 207.575656 \nL 303.051588 207.091666 \nL 303.559707 205.807951 \nL 304.067827 207.163501 \nL 304.575947 207.55885 \nL 305.084066 207.086662 \nL 305.592186 207.892941 \nL 306.100305 208.215078 \nL 307.116545 207.320027 \nL 307.624664 207.610547 \nL 308.132784 208.841395 \nL 308.640903 208.635175 \nL 309.657143 208.569471 \nL 310.165262 208.01309 \nL 311.181501 208.827948 \nL 311.689621 208.756929 \nL 312.197741 209.251314 \nL 312.70586 209.554279 \nL 313.21398 209.667055 \nL 313.722099 210.053038 \nL 314.230219 209.232668 \nL 314.738339 210.508392 \nL 315.246458 209.584454 \nL 315.754578 209.737837 \nL 316.262697 210.355261 \nL 316.770817 210.265688 \nL 317.278936 210.019055 \nL 317.787056 209.976019 \nL 318.295176 211.027664 \nL 318.803295 211.378591 \nL 319.311415 210.348333 \nL 319.819534 210.364997 \nL 320.327654 211.245626 \nL 320.835774 210.457045 \nL 321.343893 212.075178 \nL 321.852013 210.674368 \nL 322.360132 212.410531 \nL 322.868252 211.279981 \nL 323.376372 212.814598 \nL 324.90073 211.186529 \nL 325.91697 212.158345 \nL 326.425089 211.887328 \nL 327.441328 210.908381 \nL 327.949448 212.685893 \nL 328.965687 212.87893 \nL 329.473807 212.351137 \nL 329.981926 212.576318 \nL 330.490046 212.591727 \nL 330.998166 212.17321 \nL 331.506285 212.960954 \nL 332.014405 212.850574 \nL 332.522524 212.314486 \nL 333.030644 213.65747 \nL 333.538763 211.976236 \nL 334.046883 212.873748 \nL 334.555003 212.691384 \nL 335.063122 213.167917 \nL 335.571242 213.317988 \nL 336.079361 214.542445 \nL 336.587481 213.778615 \nL 337.095601 214.325769 \nL 337.60372 214.227772 \nL 338.11184 213.753022 \nL 338.619959 213.740551 \nL 339.636199 214.756364 \nL 340.144318 214.024279 \nL 340.144318 214.024279 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 20.5625 224.64 \nL 20.5625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 355.3625 224.64 \nL 355.3625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 20.5625 224.64 \nL 355.3625 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 20.5625 7.2 \nL 355.3625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pf800ec4f45\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"20.5625\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(lossvalues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 0.13406400920712788 0.04739878501170794\n",
      "1000 0.15491435083082525 0.0703701717526401\n",
      "2000 0.1488083724022453 0.06882481193965499\n",
      "3000 0.14858044513037535 0.06774217921841645\n",
      "4000 0.149780693702005 0.06821620774366059\n",
      "5000 0.14749914202397163 0.0665359239392407\n",
      "6000 0.1459624036713676 0.06545231769429219\n",
      "7000 0.15059928404150158 0.07010398255264323\n",
      "8000 0.14992694726229988 0.06950212830123988\n",
      "Total Bleu Score for 1 grams on testing pairs:  0.14914599735888706\n",
      "Total Bleu Score for 2 grams on testing pairs:  0.06901677912882646\n"
     ]
    }
   ],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "from nltk.translate.bleu_score import sentence_bleu,corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "############################################################################\n",
    "# Difference between greedy search and beam search is here\n",
    "\n",
    "# greedy search\n",
    "# searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# beam search\n",
    "searcher = BeamSearchDecoder(encoder, decoder, voc, 10)\n",
    "############################################################################\n",
    "gram1_bleu_score = []\n",
    "gram2_bleu_score = []\n",
    "for i in range(0,len(testpairs),1):\n",
    "  \n",
    "  input_sentence = testpairs[i][0]\n",
    "  \n",
    "  reference = testpairs[i][1:]\n",
    "  templist = []\n",
    "  for k in range(len(reference)):\n",
    "    if(reference[k]!=''):\n",
    "      temp = reference[k].split(' ')\n",
    "      templist.append(temp)\n",
    "  \n",
    "  \n",
    "  input_sentence = normalizeString(input_sentence)\n",
    "  output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "  output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "  chencherry = SmoothingFunction()\n",
    "  score1 = sentence_bleu(templist,output_words,weights=(1, 0, 0, 0) ,smoothing_function=chencherry.method1)\n",
    "  score2 = sentence_bleu(templist,output_words,weights=(0.5, 0.5, 0, 0),smoothing_function=chencherry.method1) \n",
    "  gram1_bleu_score.append(score1)\n",
    "  gram2_bleu_score.append(score2)\n",
    "  if i%1000 == 0:\n",
    "    print(i,sum(gram1_bleu_score)/len(gram1_bleu_score),sum(gram2_bleu_score)/len(gram2_bleu_score))\n",
    "print(\"Total Bleu Score for 1 grams on testing pairs: \", sum(gram1_bleu_score)/len(gram1_bleu_score))  \n",
    "print(\"Total Bleu Score for 2 grams on testing pairs: \", sum(gram2_bleu_score)/len(gram2_bleu_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bot: i m going to meet you .\n",
      "Bot: and what do you want to do ?\n",
      "Bot: what do you want to do ?\n",
      "Bot: what are you doing here ?\n",
      "Error: Encountered unknown word.\n",
      "Error: Encountered unknown word.\n",
      "Bot: yes ?\n",
      "Bot: how long have you been doing here ?\n",
      "Bot: how long have you been doing here ?\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-72fb9220f3db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msearcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeamSearchDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mevaluateInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-ec18c2cf8348>\u001b[0m in \u001b[0;36mevaluateInput\u001b[0;34m(encoder, decoder, searcher, voc)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0minput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'> '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'q'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m         )\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "searcher = BeamSearchDecoder(encoder, decoder, voc, 10)\n",
    "evaluateInput(encoder, decoder, searcher, voc)"
   ]
  }
 ]
}