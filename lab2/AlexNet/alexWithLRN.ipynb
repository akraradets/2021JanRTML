{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Sequntial AlexNET"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/dsai-asia/RTML/blob/main/Labs/02-PyTorch-AlexNet-GoogLeNet/02-PyTorch-AlexNet-GoogLeNet.ipynb\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we set up Dataset objects and DataLoader objects to load images, transform them to 3x224x224, and batch them for training/testing:\n",
    "\n",
    "# Set up preprocessing of CIFAR-10 images to 3x224x224 with normalization\n",
    "# using the magic ImageNet means and standard deviations. You can try\n",
    "# RandomCrop, RandomHorizontalFlip, etc. during training to obtain\n",
    "# slightly better generalization.\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "# Download CIFAR-10 and split into training, validation, and test sets\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=preprocess)\n",
    "\n",
    "# Split the training set into training and validation sets randomly.\n",
    "# CIFAR-10 train contains 50,000 examples, so let's split 80%/20%.\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [40000, 10000])\n",
    "\n",
    "# Download the test set. If you use data augmentation transforms for the training set, you'll want to use a different transformer here.\n",
    "### If we do data augment, we don't \"usually\" augment the test data ###\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='../data', train=False,download=True, transform=preprocess)\n",
    "\n",
    "# Dataset objects are mainly designed for datasets that can't fit entirely into memory.\n",
    "# Dataset objects don't load examples into memory until their __getitem__() method is\n",
    "# called. For supervised learning datasets, __getitem__() normally returns a 2-tuple\n",
    "# on each call. To make a Dataset object like this useful, we use a DataLoader object\n",
    "# to optionally shuffle then batch the examples in each dataset. During training.\n",
    "# To keep our memory utilization small, we'll use 4 images per batch, but we could use\n",
    "# a much larger batch size on a dedicated GPU. To obtain optimal usage of the GPU, we\n",
    "# would like to load the examples for the next batch while the current batch is being\n",
    "# used for training. DataLoader handles this by spawining \"worker\" threads that proactively\n",
    "# fetch the next batch in the background, enabling parallel training on the GPU and data\n",
    "# loading/transforming/augmenting on the CPU. Here we use num_workers=2 (the default)\n",
    "# so that two batches are always ready or being prepared.\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=4, shuffle=True , num_workers=2)\n",
    "val_dataloader  = torch.utils.data.DataLoader(val_dataset,   batch_size=4, shuffle=False, num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,  batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple module to flatten a batched feature map tensor into a batched vector tensor\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        return x.view(batch_size, -1)\n",
    "\n",
    "# AlexNet-like model using the Sequential API\n",
    "NUM_CLASSES = 10\n",
    "alexnet_sequential = nn.Sequential(\n",
    "    # Layer 1\n",
    "    nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),\n",
    "    nn.ReLU(inplace=True),\n",
    "    ### LRN1 ###\n",
    "    nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2.0),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    # Layer 2\n",
    "    nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
    "    nn.ReLU(inplace=True),\n",
    "    ## LRN2 ###\n",
    "    nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2.0),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    # Layer 3\n",
    "    nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    # Layer 4\n",
    "    nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    # Layer 5\n",
    "    nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    # Layer 6\n",
    "    nn.AdaptiveAvgPool2d((6, 6)),\n",
    "    Flatten(),\n",
    "    # Layer 7\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(256 * 6 * 6, 4096),\n",
    "    nn.ReLU(inplace=True),\n",
    "    # Layer 8\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(inplace=True),\n",
    "    # Layer 9\n",
    "    nn.Linear(4096, NUM_CLASSES)\n",
    ")\n",
    "\n",
    "# Move model to target device\n",
    "alexnet_sequential = alexnet_sequential.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Implement \"local response normalization\"\n",
    "# https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n",
    "# The constants k, n, α, and β are hyper-parameters whose\n",
    "# values are determined using a validation set; we used k = 2, n = 5, α = 10−4, and β = 0.75. \n",
    "# We applied this normalization after applying the ReLU nonlinearity in certain layers (see Section 3.5).\n",
    "\n",
    "nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2.0)\n",
    "\n",
    "\n",
    "# Response-normalization layers follow the first and second convolutional layers.\n",
    "# Max-pooling layers, of the kind described in Section 3.4, follow both response-normalization layers as well as the fifth convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "params_to_update = alexnet_sequential.parameters()\n",
    "# optimizer = optim.SGD(params_to_update , lr=0.001, momentum=0.9)\n",
    "optimizer = optim.AdamW(params_to_update, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, weights_name='weight_save', is_inception=False):\n",
    "    '''\n",
    "    train_model function\n",
    "\n",
    "    Train a PyTorch model for a given number of epochs.\n",
    "    \n",
    "            Parameters:\n",
    "                    model: Pytorch model\n",
    "                    dataloaders: dataset\n",
    "                    criterion: loss function\n",
    "                    optimizer: update weights function\n",
    "                    num_epochs: number of epochs\n",
    "                    weights_name: file name to save weights\n",
    "                    is_inception: The model is inception net (Google LeNet) or not\n",
    "\n",
    "            Returns:\n",
    "                    model: Best model from evaluation result\n",
    "                    val_acc_history: evaluation accuracy history\n",
    "                    loss_acc_history: loss value history\n",
    "    '''\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    loss_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over the train/validation dataset according to which phase we're in\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                # Inputs is one batch of input images, and labels is a corresponding vector of integers\n",
    "                # labeling each image in the batch. First, we move these tensors to our target device.\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # Zero out any parameter gradients that have previously been calculated. Parameter\n",
    "                # gradients accumulate over as many backward() passes as we let them, so they need\n",
    "                # to be zeroed out after each optimizer step.\n",
    "                optimizer.zero_grad()\n",
    "                # Instruct PyTorch to track gradients only if this is the training phase, then run the\n",
    "                # forward propagation and optionally the backward propagation step for this iteration.\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # The inception model is a special case during training because it has an auxiliary\n",
    "                    # output used to encourage discriminative representations in the deeper feature maps.\n",
    "                    # We need to calculate loss for both outputs. Otherwise, we have a single output to\n",
    "                    # calculate the loss on.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4 * loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    # Backpropagate only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                # Gather our summary statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            epoch_end = time.time()\n",
    "            \n",
    "            elapsed_epoch = epoch_end - epoch_start\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            print(\"Epoch time taken: \", elapsed_epoch)\n",
    "\n",
    "            # If this is the best model on the validation set so far, deep copy it\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), weights_name + \".pth\")\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "            if phase == 'train':\n",
    "                loss_acc_history.append(epoch_loss)\n",
    "\n",
    "        print()\n",
    "\n",
    "    # Output summary statistics, load the best weight set, and return results\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history, loss_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = { 'train': train_dataloader, 'val': val_dataloader }\n",
    "best_model, val_acc_history, loss_acc_history = train_model(alexnet_sequential, dataloaders, criterion, optimizer, 10, 'alex_sequential_LRN_lr_0.001_bestsofar')"
   ]
  }
 ]
}